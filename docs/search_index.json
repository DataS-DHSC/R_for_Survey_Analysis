[["index.html", "Intermediate R - R for Survey Analysis Chapter 1 Index 1.1 Disclaimer", " Intermediate R - R for Survey Analysis DHSC Analysts 2021-04-22 Chapter 1 Index This is a living document of the course notes for our training that follows from Intro to R for people familiar with SAS/SPSS who would like to analyse survey data in R. Available to all under the Open Government Licence. 1.1 Disclaimer This is an unapproved draft and does not represent the views of the DHSC. "],["introduction.html", "Chapter 2 Introduction 2.1 Aims 2.2 Why R", " Chapter 2 Introduction Course Notes for Intermediate R - R for Survey Analysis. 2.1 Aims Visualisation of the data processing workflow 2.1.1 Day 1 Load SAS/SPSS data in R. Clean the data. Exploring the data with summary stats. Applying survey weightings. Cross tabs. 2.1.2 Day 2 Plotting survey data. Regression &amp; plotting. Testing regression assumptions and hypothesis testing. 2.2 Why R (Youre here so hopefully youve been persuaded.) No licensing issues. More than 8 people can use it at once. Robustness - I can share the course notes and know it runs the same on your computer. Point-and-click SAS/SPSS has failed some scientific papers. Reuse - when solving the same problem you can copy-paste existing code. "],["day-1-1.html", "Chapter 3 Day 1 3.1 Outline for Day 1", " Chapter 3 Day 1 3.1 Outline for Day 1 We will cover: Loading survey data. Cleaning the data. Exploring the data. Applying survey weightings. Cross tabs. "],["loading-survey-data.html", "Chapter 4 Loading Survey Data 4.1 Analysing a survey in R 4.2 Loading the data 4.3 Cleaning the data 4.4 Exploring the data 4.5 Applying survey weighting for exploratory stats", " Chapter 4 Loading Survey Data 4.1 Analysing a survey in R Further Reading: R for Data Science, Hadley. 4.2 Loading the data As an example dataset well use the CDC National Health &amp; Nutrition Examination Survey. Its American, but its easier to access than the Health Survey for England. In RStudio create a new project, start a new script, and create a data/ folder. Download the demographic data file and the Body Measures data file to your data folder. Well load some libraries and the demographic data: library(tidyverse) library(haven) library(janitor) # Load demographic data nhanes &lt;- read_xpt(&quot;data/DEMO_J.XPT&quot;) And look at the first few rows: slice_head(nhanes, n=10) %&gt;% View() slice_head(nhanes, n=10) %&gt;% DT::datatable() We need the data dictionary to make sense of this. 4.3 Cleaning the data Cleaning data is long, and repetitive. best practise: clean it once, share the clean data. Example good-enough practise: keep the columns youre interested in, clean those. For 1-off analysis (2) is fair and proportionate. For weekly/monthly stats (1) is better - talk to Data Science team about RAP. 4.4 Exploring the data Weve already explored the data a little with View. This is perfectly valid. Hypothetical scenario - a stakeholder wants to know if targeting weight management services at demographics with lower education levels might improve health inequalities. Education level is in the demographics table, BMI is in the examination table. We want education &amp; participant ID from demographics, to join it with BMI &amp; participant ID from examinations. (Adult) education level is held in column DMDEDUC2. # recode Adult education nhanes &lt;- nhanes %&gt;% mutate(Education = case_when( DMDEDUC2 == 1 ~ &quot;Less than 9th grade&quot;, DMDEDUC2 == 2 ~ &quot;9-11th grade (Includes 12th grade with no diploma)&quot;, DMDEDUC2 == 3 ~ &quot;High school graduate/GED or equivalent&quot;, DMDEDUC2 == 4 ~ &quot;Some college or AA degree&quot;, DMDEDUC2 == 5 ~ &quot;College graduate or above&quot;, DMDEDUC2 == 7 ~ &quot;Refused&quot;, DMDEDUC2 == 9 ~ &quot;Don&#39;t Know&quot; )) %&gt;% select(ID = SEQN, Education) nhanes %&gt;% slice_head(n=10) %&gt;% DT::datatable() data dictionary for examination dataset # Load examination data exam &lt;- read_xpt(&quot;data/BMX_J.XPT&quot;) %&gt;% select(ID = SEQN, BMI = BMXBMI) exam %&gt;% slice_head(n = 10) %&gt;% DT::datatable() Joining them on ID: Refresher on joins nhanes &lt;- left_join(nhanes, exam, by=&quot;ID&quot;) nhanes %&gt;% slice_head(n=10) %&gt;% DT::datatable(nhanes) Keeping people with Education level recorded, &amp; valid BMI: nhanes %&gt;% filter(!is.na(Education), !is.na(BMI)) %&gt;% select(-ID) %&gt;% group_by(Education) %&gt;% summarise(average_BMI = mean(BMI)) %&gt;% knitr::kable() Education average_BMI 9-11th grade (Includes 12th grade with no diploma) 29.27825 College graduate or above 28.50249 Dont Know 31.11250 High school graduate/GED or equivalent 30.13217 Less than 9th grade 29.93982 Refused 30.20000 Some college or AA degree 30.82326 Refresh on filter Refresh on select Refresh on grouping &amp; summarising No obvious relationship there, but I didnt apply the survey weighting. 4.5 Applying survey weighting for exploratory stats In reality someone has tidied the NHANES data for R, so Ill load that. rm(exam, nhanes) # We&#39;re not using these data any more, we can remove them from memory. nhanes &lt;- NHANES::NHANESraw nhanes %&gt;% slice_head(n=10) %&gt;% DT::datatable() The survey weighting is WTMEC2YR, and we can summarise with weighted.mean.: nhanes %&gt;% filter(!is.na(Education), !is.na(BMI)) %&gt;% group_by(Education) %&gt;% summarise(average_BMI = weighted.mean(BMI, WTMEC2YR)) %&gt;% knitr::kable() Education average_BMI 8th Grade 29.22906 9 - 11th Grade 29.20260 High School 29.40650 Some College 29.17616 College Grad 27.50059 The manual page for weighted.mean can be viewed with ?weighted.mean or F1 when the cursor is inside weighted.mean. It looks like theres a distinction between college grads and non-college grads. "],["crosstabs.html", "Chapter 5 Crosstabs 5.1 Case 1: with survey weights 5.2 Case 2: Without survey weights", " Chapter 5 Crosstabs Two cases: 5.1 Case 1: with survey weights nhanes %&gt;% select(Gender, Education, wt=WTINT2YR) %&gt;% # There&#39;s a clever way to do this, but it&#39;s easier to write this 3 times. filter(!is.na(Gender), !is.na(wt), !is.na(Education)) %&gt;% group_by(Gender, Education) %&gt;% summarise(wt = sum(wt)) %&gt;% group_by(Gender) %&gt;% mutate(wt = scales::percent(wt/sum(wt))) %&gt;% pivot_wider(id_cols = c(Gender, Education), names_from=Gender, values_from=wt) %&gt;% knitr::kable() ## `summarise()` has grouped output by &#39;Gender&#39;. You can override using the `.groups` argument. Education female male 8th Grade 5.9% 6.24% 9 - 11th Grade 11.4% 12.14% High School 20.4% 22.62% Some College 32.8% 29.73% College Grad 29.4% 29.28% 5.2 Case 2: Without survey weights Using the janitor package. nhanes %&gt;% filter(!is.na(Education), !is.na(Gender)) %&gt;% tabyl(Education, Gender) %&gt;% adorn_percentages(&quot;col&quot;) %&gt;% adorn_pct_formatting(digits = 2) %&gt;% knitr::kable() Education female male 8th Grade 11.01% 11.47% 9 - 11th Grade 14.77% 15.65% High School 20.83% 23.37% Some College 31.00% 26.72% College Grad 22.39% 22.80% "],["day-2-1.html", "Chapter 6 Day 2 6.1 Outline for Day 2", " Chapter 6 Day 2 6.1 Outline for Day 2 Disclaimer: The focus of the training is not on understanding the statistical concepts, but rather on how to implement common tasks in R - with a few stats sprinkles. Im trying to show you the quick &amp; basic way of doing this and give you an idea of the fancier approaches - which gives you more flexibility (but also has a steeper learning curve). You can of course dive deeper on those topics or request follow-up training. :) Today we will cover the following topics: plotting (base R, ggplot2) regression models testing regression assumptions significance tests (chi2, t-test) 6.1.1 Take home messages to start with Work on a copy of your data in R - do not touch the raw data Work in R projects Structure your code with sections (Ctr shift R) help(x) or ?x is your friend when trying to understand function x scroll down to Examples in the helper window that pops up # load all required packages pacman::p_load(dplyr, readr, ggplot2, corrplot, NHANES, survey, broom, lmtest, car) # inspect data head(nhanes) ## # A tibble: 6 x 78 ## ID SurveyYr Gender Age AgeMonths Race1 Race3 Education ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 51624 2009_10 male 34 409 White &lt;NA&gt; High School ## 2 51625 2009_10 male 4 49 Other &lt;NA&gt; &lt;NA&gt; ## 3 51626 2009_10 male 16 202 Black &lt;NA&gt; &lt;NA&gt; ## 4 51627 2009_10 male 10 131 Black &lt;NA&gt; &lt;NA&gt; ## 5 51628 2009_10 female 60 722 Black &lt;NA&gt; High School ## 6 51629 2009_10 male 26 313 Mexican &lt;NA&gt; 9 - 11th Gra~ ## # ... with 70 more variables: MaritalStatus &lt;fct&gt;, HHIncome &lt;fct&gt;, ## # HHIncomeMid &lt;int&gt;, Poverty &lt;dbl&gt;, HomeRooms &lt;int&gt;, ## # HomeOwn &lt;fct&gt;, Work &lt;fct&gt;, Weight &lt;dbl&gt;, Length &lt;dbl&gt;, ## # HeadCirc &lt;dbl&gt;, Height &lt;dbl&gt;, BMI &lt;dbl&gt;, ## # BMICatUnder20yrs &lt;fct&gt;, BMI_WHO &lt;fct&gt;, Pulse &lt;int&gt;, ## # BPSysAve &lt;int&gt;, BPDiaAve &lt;int&gt;, BPSys1 &lt;int&gt;, BPDia1 &lt;int&gt;, ## # BPSys2 &lt;int&gt;, BPDia2 &lt;int&gt;, BPSys3 &lt;int&gt;, BPDia3 &lt;int&gt;, ## # Testosterone &lt;dbl&gt;, DirectChol &lt;dbl&gt;, TotChol &lt;dbl&gt;, ## # UrineVol1 &lt;int&gt;, UrineFlow1 &lt;dbl&gt;, UrineVol2 &lt;int&gt;, ## # UrineFlow2 &lt;dbl&gt;, Diabetes &lt;fct&gt;, DiabetesAge &lt;int&gt;, ## # HealthGen &lt;fct&gt;, DaysPhysHlthBad &lt;int&gt;, DaysMentHlthBad &lt;int&gt;, ## # LittleInterest &lt;fct&gt;, Depressed &lt;fct&gt;, nPregnancies &lt;int&gt;, ## # nBabies &lt;int&gt;, Age1stBaby &lt;int&gt;, SleepHrsNight &lt;int&gt;, ## # SleepTrouble &lt;fct&gt;, PhysActive &lt;fct&gt;, PhysActiveDays &lt;int&gt;, ## # TVHrsDay &lt;fct&gt;, CompHrsDay &lt;fct&gt;, TVHrsDayChild &lt;int&gt;, ## # CompHrsDayChild &lt;int&gt;, Alcohol12PlusYr &lt;fct&gt;, ## # AlcoholDay &lt;int&gt;, AlcoholYear &lt;int&gt;, SmokeNow &lt;fct&gt;, ## # Smoke100 &lt;fct&gt;, SmokeAge &lt;int&gt;, Marijuana &lt;fct&gt;, ## # AgeFirstMarij &lt;int&gt;, RegularMarij &lt;fct&gt;, AgeRegMarij &lt;int&gt;, ## # HardDrugs &lt;fct&gt;, SexEver &lt;fct&gt;, SexAge &lt;int&gt;, ## # SexNumPartnLife &lt;int&gt;, SexNumPartYear &lt;int&gt;, SameSex &lt;fct&gt;, ## # SexOrientation &lt;fct&gt;, WTINT2YR &lt;dbl&gt;, WTMEC2YR &lt;dbl&gt;, ## # SDMVPSU &lt;int&gt;, SDMVSTRA &lt;int&gt;, PregnantNow &lt;fct&gt; dplyr::glimpse(nhanes) # if there are many columns (from dplyr package) ## Rows: 20,293 ## Columns: 78 ## $ ID &lt;int&gt; 51624, 51625, 51626, 51627, 51628, 51629~ ## $ SurveyYr &lt;fct&gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009~ ## $ Gender &lt;fct&gt; male, male, male, male, female, male, fe~ ## $ Age &lt;int&gt; 34, 4, 16, 10, 60, 26, 49, 1, 10, 80, 10~ ## $ AgeMonths &lt;int&gt; 409, 49, 202, 131, 722, 313, 596, 12, 12~ ## $ Race1 &lt;fct&gt; White, Other, Black, Black, Black, Mexic~ ## $ Race3 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ Education &lt;fct&gt; High School, NA, NA, NA, High School, 9 ~ ## $ MaritalStatus &lt;fct&gt; Married, NA, NA, NA, Widowed, Married, L~ ## $ HHIncome &lt;fct&gt; 25000-34999, 20000-24999, 45000-54999, 2~ ## $ HHIncomeMid &lt;int&gt; 30000, 22500, 50000, 22500, 12500, 30000~ ## $ Poverty &lt;dbl&gt; 1.36, 1.07, 2.27, 0.81, 0.69, 1.01, 1.91~ ## $ HomeRooms &lt;int&gt; 6, 9, 5, 6, 6, 4, 5, 5, 7, 4, 5, 5, 7, N~ ## $ HomeOwn &lt;fct&gt; Own, Own, Own, Rent, Rent, Rent, Rent, R~ ## $ Work &lt;fct&gt; NotWorking, NA, NotWorking, NA, NotWorki~ ## $ Weight &lt;dbl&gt; 87.4, 17.0, 72.3, 39.8, 116.8, 97.6, 86.~ ## $ Length &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 75.7, NA, NA~ ## $ HeadCirc &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ Height &lt;dbl&gt; 164.7, 105.4, 181.3, 147.8, 166.0, 173.0~ ## $ BMI &lt;dbl&gt; 32.22, 15.30, 22.00, 18.22, 42.39, 32.61~ ## $ BMICatUnder20yrs &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ BMI_WHO &lt;fct&gt; 30.0_plus, 12.0_18.5, 18.5_to_24.9, 12.0~ ## $ Pulse &lt;int&gt; 70, NA, 68, 68, 72, 72, 86, NA, 70, 88, ~ ## $ BPSysAve &lt;int&gt; 113, NA, 109, 93, 150, 104, 112, NA, 108~ ## $ BPDiaAve &lt;int&gt; 85, NA, 59, 41, 68, 49, 75, NA, 53, 43, ~ ## $ BPSys1 &lt;int&gt; 114, NA, 112, 92, 154, 102, 118, NA, 106~ ## $ BPDia1 &lt;int&gt; 88, NA, 62, 36, 70, 50, 82, NA, 60, 62, ~ ## $ BPSys2 &lt;int&gt; 114, NA, 114, 94, 150, 104, 108, NA, 106~ ## $ BPDia2 &lt;int&gt; 88, NA, 60, 44, 68, 48, 74, NA, 50, 46, ~ ## $ BPSys3 &lt;int&gt; 112, NA, 104, 92, 150, 104, 116, NA, 110~ ## $ BPDia3 &lt;int&gt; 82, NA, 58, 38, 68, 50, 76, NA, 56, 40, ~ ## $ Testosterone &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ DirectChol &lt;dbl&gt; 1.29, NA, 1.55, 1.89, 1.16, 1.16, 1.16, ~ ## $ TotChol &lt;dbl&gt; 3.49, NA, 4.97, 4.16, 5.22, 4.14, 6.70, ~ ## $ UrineVol1 &lt;int&gt; 352, NA, 281, 139, 30, 202, 77, NA, 39, ~ ## $ UrineFlow1 &lt;dbl&gt; NA, NA, 0.415, 1.078, 0.476, 0.563, 0.09~ ## $ UrineVol2 &lt;int&gt; NA, NA, NA, NA, 246, NA, NA, NA, NA, NA,~ ## $ UrineFlow2 &lt;dbl&gt; NA, NA, NA, NA, 2.51, NA, NA, NA, NA, NA~ ## $ Diabetes &lt;fct&gt; No, No, No, No, Yes, No, No, No, No, No,~ ## $ DiabetesAge &lt;int&gt; NA, NA, NA, NA, 56, NA, NA, NA, NA, NA, ~ ## $ HealthGen &lt;fct&gt; Good, NA, Vgood, NA, Fair, Good, Good, N~ ## $ DaysPhysHlthBad &lt;int&gt; 0, NA, 2, NA, 20, 2, 0, NA, NA, 0, NA, 0~ ## $ DaysMentHlthBad &lt;int&gt; 15, NA, 0, NA, 25, 14, 10, NA, NA, 0, NA~ ## $ LittleInterest &lt;fct&gt; Most, NA, NA, NA, Most, None, Several, N~ ## $ Depressed &lt;fct&gt; Several, NA, NA, NA, Most, Most, Several~ ## $ nPregnancies &lt;int&gt; NA, NA, NA, NA, 1, NA, 2, NA, NA, NA, NA~ ## $ nBabies &lt;int&gt; NA, NA, NA, NA, 1, NA, 2, NA, NA, NA, NA~ ## $ Age1stBaby &lt;int&gt; NA, NA, NA, NA, NA, NA, 27, NA, NA, NA, ~ ## $ SleepHrsNight &lt;int&gt; 4, NA, 8, NA, 4, 4, 8, NA, NA, 6, NA, 9,~ ## $ SleepTrouble &lt;fct&gt; Yes, NA, No, NA, No, No, Yes, NA, NA, No~ ## $ PhysActive &lt;fct&gt; No, NA, Yes, NA, No, Yes, No, NA, NA, Ye~ ## $ PhysActiveDays &lt;int&gt; NA, NA, 5, NA, NA, 2, NA, NA, NA, 4, NA,~ ## $ TVHrsDay &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ CompHrsDay &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ TVHrsDayChild &lt;int&gt; NA, 4, NA, 1, NA, NA, NA, NA, 1, NA, 3, ~ ## $ CompHrsDayChild &lt;int&gt; NA, 1, NA, 1, NA, NA, NA, NA, 0, NA, 0, ~ ## $ Alcohol12PlusYr &lt;fct&gt; Yes, NA, NA, NA, No, Yes, Yes, NA, NA, Y~ ## $ AlcoholDay &lt;int&gt; NA, NA, NA, NA, NA, 19, 2, NA, NA, 1, NA~ ## $ AlcoholYear &lt;int&gt; 0, NA, NA, NA, 0, 48, 20, NA, NA, 52, NA~ ## $ SmokeNow &lt;fct&gt; No, NA, NA, NA, Yes, No, Yes, NA, NA, No~ ## $ Smoke100 &lt;fct&gt; Yes, NA, NA, NA, Yes, Yes, Yes, NA, NA, ~ ## $ SmokeAge &lt;int&gt; 18, NA, NA, NA, 16, 15, 38, NA, NA, 16, ~ ## $ Marijuana &lt;fct&gt; Yes, NA, NA, NA, NA, Yes, Yes, NA, NA, N~ ## $ AgeFirstMarij &lt;int&gt; 17, NA, NA, NA, NA, 10, 18, NA, NA, NA, ~ ## $ RegularMarij &lt;fct&gt; No, NA, NA, NA, NA, Yes, No, NA, NA, NA,~ ## $ AgeRegMarij &lt;int&gt; NA, NA, NA, NA, NA, 12, NA, NA, NA, NA, ~ ## $ HardDrugs &lt;fct&gt; Yes, NA, NA, NA, No, Yes, Yes, NA, NA, N~ ## $ SexEver &lt;fct&gt; Yes, NA, NA, NA, Yes, Yes, Yes, NA, NA, ~ ## $ SexAge &lt;int&gt; 16, NA, NA, NA, 15, 9, 12, NA, NA, NA, N~ ## $ SexNumPartnLife &lt;int&gt; 8, NA, NA, NA, 4, 10, 10, NA, NA, NA, NA~ ## $ SexNumPartYear &lt;int&gt; 1, NA, NA, NA, NA, 1, 1, NA, NA, NA, NA,~ ## $ SameSex &lt;fct&gt; No, NA, NA, NA, No, No, Yes, NA, NA, NA,~ ## $ SexOrientation &lt;fct&gt; Heterosexual, NA, NA, NA, NA, Heterosexu~ ## $ WTINT2YR &lt;dbl&gt; 80100.544, 53901.104, 13953.078, 11664.8~ ## $ WTMEC2YR &lt;dbl&gt; 81528.772, 56995.035, 14509.279, 12041.6~ ## $ SDMVPSU &lt;int&gt; 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2~ ## $ SDMVSTRA &lt;int&gt; 83, 79, 84, 86, 75, 88, 85, 86, 88, 77, ~ ## $ PregnantNow &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ # get a feeling for variables # categorical levels(nhanes$Depressed) ## [1] &quot;None&quot; &quot;Several&quot; &quot;Most&quot; table(nhanes$Depressed, useNA = &quot;ifany&quot;) ## ## None Several Most &lt;NA&gt; ## 7926 1774 814 9779 # numerical summary(nhanes$BMI, useNA = &quot;ifany&quot;) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 12.40 19.79 24.92 25.65 30.10 84.87 2279 "],["plotting-basic-charts-base-r.html", "Chapter 7 Plotting basic charts (base R) 7.1 histograms - basic frequencies 7.2 bar charts - plotting stats across categories 7.3 Box plots - plotting distribution of several categories/vars 7.4 Scatter plots - relationship between two continuous vars 7.5 Tiny statistics excursion 7.6 Are there any other plots that you regularly use? ", " Chapter 7 Plotting basic charts (base R) It is important to distinguish between exploratory graphs and explanatory graphs: * Exploratory is done as part of analysis and there is no need to be pretty * Explanatory graphs are done once we understand the data and want to get insights across (for sharing with others) 7.1 histograms - basic frequencies # basic frequencies for numerical variables hist(nhanes$Height) hist(nhanes$Age) hist(nhanes$Age[nhanes$Age&lt;10]) # they sampled for both 0-6months and 6-12months 7.2 bar charts - plotting stats across categories # Great for plotting a statistic (e.g. mean value) for categorical data # Get dataframe into right format aggregates_by_health &lt;- nhanes %&gt;% dplyr::group_by(HealthGen) %&gt;% dplyr::summarise(count = n(), mean_weight = round(mean(Weight, na.rm=T),1), mean_age = round(mean(Age, na.rm=T),1)) # create bar plot to plot either Frequencies or aggregate statistics # Participant numbers by race barplot(count ~ HealthGen, data = aggregates_by_health) # average weight and age by race barplot(mean_weight ~ HealthGen, data = aggregates_by_health) 7.3 Box plots - plotting distribution of several categories/vars # Good for distribution of continuous data across categories boxplot(nhanes$Age ~ HealthGen, data = nhanes ) 7.4 Scatter plots - relationship between two continuous vars # Let&#39;s keep only adults in for this adults &lt;- nhanes %&gt;% dplyr::filter(Age &gt;= 18) # plot scatter plot plot(x = adults$Height, y = adults$Weight) Can you guess the Correlation coefficient? Whats the value of the correlation coefficient?\" cor(adults$Height, adults$Weight, use = &quot;pairwise.complete.obs&quot;) ## [1] 0.434507 Now lets add the line of best fit. First, we calculate slope and intercept of line of best fit coef(lm(Weight ~ Height, data = adults)) ## (Intercept) Height ## -72.209450 0.915329 Then we can add them to the plot # add them to plot (run both commands) plot(x = adults$Height, y = adults$Weight) # intercept and slope abline(-72.209450 , 0.915329) 7.5 Tiny statistics excursion Whats the relationship between the linear model regression coefficient and the correlation coefficient? hnorm &lt;- adults$Height/sd(adults$Height, na.rm = TRUE) wnorm &lt;- adults$Weight/sd(adults$Weight, na.rm = TRUE) df_norm &lt;- as.data.frame(cbind(hnorm,wnorm)) coef(lm(hnorm ~ wnorm, data = df_norm)) ## (Intercept) wnorm ## 14.7794973 0.4345723 Note: For correlation coef, the larger the value, the stronger the (linear!) relationship. For regression coefficients, a larger slope coefficient does NOT imply that the association between variables is stronger (remember the unit dependency). 7.6 Are there any other plots that you regularly use? "],["intermediate-plotting-in-r-ggplot2.html", "Chapter 8 Intermediate plotting in R (GGPLOT2) 8.1 Adding transparency 8.2 Adding automatic line of best fit 8.3 Adding colours 8.4 Fitting a line of best fit for each group of a categorical variable 8.5 Exercise: Brief in-class practice of making charts 8.6 ggplot cheat sheet", " Chapter 8 Intermediate plotting in R (GGPLOT2) This was hopefully all very straight forward and the code was easy to read. In the R universe new packages are created all the time GGPLOT2 is THE data viz package in R More customisable ways of plotting - in R people use a package called ggplot2 we can add layer, upon layer of extra info and change transparency and colours in a nutshell we have DATA, AESTHETICS, GEOMETRIES. Lets have a look at what this means in practice. 8.1 Adding transparency # 2.1 Transparency ggplot(data = adults, aes(x = Height, y = Weight)) + geom_point(alpha = 0.4) ## Warning: Removed 580 rows containing missing values (geom_point). 8.2 Adding automatic line of best fit # 2.2 scatter plot with automatic line of best fit ggplot(data = adults, aes(x = Height, y = Weight)) + geom_point(alpha = 0.4) + geom_smooth(method = &quot;lm&quot;, se = FALSE) # se = TRUE would show standard error bars along the line 8.3 Adding colours Other cool things you can easily / automatically do with ggplot include colouring by category: # Define default colour scale suitable for colour-blind users scale_colour_discrete &lt;- ggthemes::scale_color_colorblind # plot in different colours based on the Gender variable ggplot(data = adults, aes(x = Height, y = Weight, colour = Gender)) + geom_point(alpha = 0.4) 8.4 Fitting a line of best fit for each group of a categorical variable # 2.4 get best fit line by Species ggplot(data = adults, aes(x = Height, y = Weight, color = Gender)) + geom_point(alpha = 0.4) + geom_smooth(method = &quot;lm&quot;, se = FALSE) 8.5 Exercise: Brief in-class practice of making charts Try out the simple commands for the charts that you most frequently produce in SPSS / SAS. Then choose a variable of interest that is continuous (e.g. Height or Pulse) and produce a box plot showing a break-down of distribution by a categorical variable of interest (e.g. HealthGen, MaritalStatus). 8.6 ggplot cheat sheet If youre lost or want to explore more options for plotting charts, you can consult a ggplot cheat sheet, e.g. https://www.rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf "],["regression-model.html", "Chapter 9 Regression model 9.1 Simple linear regression 9.2 Multiple linear regression 9.3 Lunch Break 9.4 ggplot cheat sheet reminder: 9.5 Regression output inspection (cont.)", " Chapter 9 Regression model 9.1 Simple linear regression The basic regression, i.e. linear model (lm), formula in R is: lm(formula = y ~ x, data = data_frame_name). Lets run a basic model to model BMI as a function of the Poverty indicator: # call model lm(BMI ~ Poverty, data = nhanes) ## ## Call: ## lm(formula = BMI ~ Poverty, data = nhanes) ## ## Coefficients: ## (Intercept) Poverty ## 25.2187 0.1856 Always save the model object - it allows you to access more of the outputs that are produced automatically when running the regression. If you only need the coefficients, call the coef function model_bmi_pov &lt;- lm(BMI ~ Poverty, data = nhanes) coef(model_bmi_pov) ## (Intercept) Poverty ## 25.2186624 0.1856308 Call the summary function on the model to see not only coefficients, but also the R-squared value, the associated standard error and p-value for each coefficient, the adjusted and the residual standard error. summary(model_bmi_pov) ## ## Call: ## lm(formula = BMI ~ Poverty, data = nhanes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.547 -5.896 -0.757 4.467 58.887 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.21866 0.10389 242.738 &lt; 2e-16 *** ## Poverty 0.18563 0.03747 4.954 7.36e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.784 on 16426 degrees of freedom ## (3865 observations deleted due to missingness) ## Multiple R-squared: 0.001492, Adjusted R-squared: 0.001431 ## F-statistic: 24.54 on 1 and 16426 DF, p-value: 7.36e-07 Inspecting model outputs: Once we have run a model, we often want to explore the fitted values and residuals (remember mean of resid = 0 and mean of fitted values = mean of actual y data). They are stored as part of the model object an can easily be accessed individually: #resid residuals(model_bmi_pov)[1:5] ## 1 2 3 4 5 ## 6.748880 -10.117287 -3.640044 -7.149023 17.043252 # fitted values fitted.values(model_bmi_pov)[1:5] ## 1 2 3 4 5 ## 25.47112 25.41729 25.64004 25.36902 25.34675 Explore all that is saved with your model To have a look at all the different aspects that are saved as part of your model, scroll down to Value section for list of all things part of your model: ?lm() 9.2 Multiple linear regression Bulding on the simple linear regression formula, the mulitple linear regression follows suit: lm(formula = y ~ x1 + x2 + x3, data = data_frame_name) # 1. save model - allows you to access more of the outputs model_mult &lt;- lm(BMI ~ Poverty + PhysActiveDays + SleepHrsNight + Diabetes + SmokeNow + Gender, data = adults) # 2. if you only need the coefficients, call the &#39;coef&#39; function coef(model_mult) ## (Intercept) Poverty PhysActiveDays SleepHrsNight ## 31.5376229 -0.1918165 -0.1567075 -0.2390670 ## DiabetesYes SmokeNowYes Gendermale ## 2.9612169 -1.3255010 -0.8920421 # 3. Or call &#39;summary&#39; for a complete output summary(model_mult) ## ## Call: ## lm(formula = BMI ~ Poverty + PhysActiveDays + SleepHrsNight + ## Diabetes + SmokeNow + Gender, data = adults) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.682 -4.064 -0.695 3.216 33.250 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.53762 0.78469 40.191 &lt; 2e-16 *** ## Poverty -0.19182 0.08205 -2.338 0.019500 * ## PhysActiveDays -0.15671 0.06908 -2.268 0.023412 * ## SleepHrsNight -0.23907 0.09518 -2.512 0.012091 * ## DiabetesYes 2.96122 0.41912 7.065 2.22e-12 *** ## SmokeNowYes -1.32550 0.28006 -4.733 2.37e-06 *** ## Gendermale -0.89204 0.26941 -3.311 0.000946 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.803 on 1954 degrees of freedom ## (10430 observations deleted due to missingness) ## Multiple R-squared: 0.04844, Adjusted R-squared: 0.04552 ## F-statistic: 16.58 on 6 and 1954 DF, p-value: &lt; 2.2e-16 9.3 Lunch Break Try to answer the following questions drawing on what you have learned about creating tables, plots and regression analysis. Is the age in months variable actually the age in months, or just derived from age in years? How is age related to sleep? Which of these variables is most strongly associated with BMI in adults: age, poverty, or number of hours sleep? 3.b And how much of the variance in BMI can be explained through these variables? If youd like to go a bit further: Explore the dataset that we have provided to you and see if you can find variables that you are interested in. Try making a histogram, boxplot, scatter plot. Next pick a continuous variable that could contribute to overall reported XXX and run a linear model. Then plot the model (i.e. a scatter plot and a line of best fit). Also report how much of the variance in XXX is explained through your predictor variable. 9.4 ggplot cheat sheet reminder: If youre lost or want to explore more options for plotting charts, you can consult a ggplot cheat sheet, e.g. https://www.rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf 9.5 Regression output inspection (cont.) In the tidy universe of R there are a few functions that make your life easier &amp; prettier (after a steeper learning curve). In the broom package there are a few helpful functions that give you objects that you can not only read, but transform or save as neat csv files. The tidy function gives you a data frame of the coefficients (&amp; confidence intervals if you so choose) at the confidence level of your liking. # 1. save model - allows you to access all model outputs model_mult &lt;- lm(BMI ~ Poverty + PhysActiveDays + SleepHrsNight + Diabetes + SmokeNow + Gender, data = adults) # 2. Create tidy output broom::tidy(model_mult, conf.int = FALSE, conf.level = 0.95) ## # A tibble: 7 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 31.5 0.785 40.2 6.09e-258 ## 2 Poverty -0.192 0.0821 -2.34 1.95e- 2 ## 3 PhysActiveDays -0.157 0.0691 -2.27 2.34e- 2 ## 4 SleepHrsNight -0.239 0.0952 -2.51 1.21e- 2 ## 5 DiabetesYes 2.96 0.419 7.07 2.22e- 12 ## 6 SmokeNowYes -1.33 0.280 -4.73 2.37e- 6 ## 7 Gendermale -0.892 0.269 -3.31 9.46e- 4 The glance function gives you the model-level metrics: # Create tidy output broom::glance(model_mult) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0484 0.0455 5.80 16.6 1.00e-18 6 -6227. ## # ... with 5 more variables: AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, ## # df.residual &lt;int&gt;, nobs &lt;int&gt; Now we can save the tidy output to easily further manipulate it or save it as a csv file. # Store tidy regression model output as object in R model_mult_tidy &lt;- broom::tidy(model_mult, conf.int = FALSE, conf.level = 0.95) # Save as a csv file at your chosen location (in &quot;outputs&quot; folder) and name in a meaningful way (e.g. BMI_model_output_p0.05.csv) readr::write_csv(model_mult_tidy, file = &quot;./outputs/BMI_model_output_p0.05.csv&quot;) 9.5.1 Try this for yourselves Using either your last regression model from the lunch break or a new one, play around with the broom package and try writing the outputs that you are interested in. Try including and excluding the confidence intervals and using different significance levels. "],["significance-tests.html", "Chapter 10 Significance tests 10.1 Chi-square 10.2 T-test", " Chapter 10 Significance tests 10.1 Chi-square Chi-square test for categorical variables determines whether there is a difference in the population proportions between two or more groups Lets look at smoking for men vs. women contingency_table_gender &lt;- table(adults$Gender, adults$SmokeNow) contingency_table_gender ## ## No Yes ## female 1116 1032 ## male 1663 1422 Or the dplyr way: adults %&gt;% dplyr::filter(!is.na(SmokeNow)) %&gt;% dplyr::group_by(Gender, SmokeNow) %&gt;% dplyr::summarise(n = n()) %&gt;% dplyr::mutate(freq = n / sum(n)) ## `summarise()` has grouped output by &#39;Gender&#39;. You can override using the `.groups` argument. ## # A tibble: 4 x 4 ## # Groups: Gender [2] ## Gender SmokeNow n freq ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 female No 1116 0.520 ## 2 female Yes 1032 0.480 ## 3 male No 1663 0.539 ## 4 male Yes 1422 0.461 chisq.test(adults$SmokeNow, adults$Gender) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity ## correction ## ## data: adults$SmokeNow and adults$Gender ## X-squared = 1.8573, df = 1, p-value = 0.1729 10.2 T-test You can specify whether you need it to be one-sided / two-sided &amp; one-sample / independent-sample. Lets look at whether BMI differs between men and women: hist(adults$BMI[adults$Gender==&quot;female&quot;]) hist(adults$BMI[adults$Gender==&quot;male&quot;]) Or lets do it the ggplot2 way # Define default colour scale suitable for colour-blind users scale_colour_discrete &lt;- ggthemes::scale_color_colorblind # Create plot facetted by Gender ggplot(adults, aes(BMI, fill = Gender)) + geom_histogram() + facet_wrap(.~Gender) ## Warning: Removed 580 rows containing non-finite values (stat_bin). Any guesses on whether there is a significant difference? Now its time to do the test. t.test(x = adults$BMI[adults$Gender == &quot;female&quot;], y = adults$BMI[adults$Gender == &quot;male&quot;], alternative = &quot;two.sided&quot;, # could also be &#39;less&#39; or &#39;greater&#39; for one-sided test paired = FALSE) ## ## Welch Two Sample t-test ## ## data: adults$BMI[adults$Gender == &quot;female&quot;] and adults$BMI[adults$Gender == &quot;male&quot;] ## t = 7.1026, df = 11386, p-value = 1.297e-12 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.6484638 1.1428217 ## sample estimates: ## mean of x mean of y ## 29.26902 28.37338 Yes, on average women seem to have a higher BMI in this survey. "],["testing-regression-assumptions.html", "Chapter 11 Testing regression assumptions 11.1 Linear relationship 11.2 normality of residuals 11.3 Testing the Homoscedasticity Assumption 11.4 Test for autocorrelation (Violations of independence) 11.5 Collinearity 11.6 Multicollinearity", " Chapter 11 Testing regression assumptions In R, regression diagnostics plots (residual diagnostics plots) can be created using the base R function plot(). # define model simple_model &lt;- lm(Weight ~ Height, data = adults) # Change the panel layout to 2 x 2 (to look at all 4 plots at once) par(mfrow = c(2, 2)) # Use plot() function to create diagnostic plots plot(simple_model) The diagnostic plots show residuals in four different ways: Residuals vs Fitted: is used to check the assumptions of linearity. If the residuals are spread equally around a horizontal line without distinct patterns (red line is approximately horizontal at zero), that is a good indication of having a linear relationship. Normal Q-Q: is used to check the normality of residuals assumption. If the majority of the residuals follow the straight dashed line, then the assumption is fulfilled. Scale-Location: is used to check the homoscedasticity of residuals (equal variance of residuals). If the residuals are spread randomly and the see a horizontal line with equally (randomly) spread points, then the assumption is fulfilled. Residuals vs Leverage: is used to identify any influential value in our dataset. Influential values are extreme values that might influence the regression results when included or excluded from the analysis. Look for cases outside of a dashed line. 11.1 Linear relationship Ideally we want a horizontal line around zero: # Create the first diagnostic plot plot(simple_model,1) # Or plot the observed versus predicted values (again ideally a horizontal line) plot(simple_model$fitted.values, simple_model$model$BMI) Looks good so far. 11.2 normality of residuals Histogram of residuals hist(simple_model$residuals) QQ Plot of Residuals The QQ plot of residuals can be used to visually check the normality assumption. The normal probability plot of residuals should approximately follow a straight line. A bow-shaped pattern of deviations from the diagonal indicates that the residuals have excessive skewness (i.e., they are not symmetrically distributed, with too many large errors in the same direction). An S-shaped pattern of deviations indicates that the residuals have excessive kurtosisi.e., there are either too many or too few large errors in both directions. plot(simple_model, 2) The majority of the points fall approximately along the reference line, so we can assume normality. The endpoints are deviating from the straight line, suggesting a heavy-tailed distribution (Distribution is longer and tails are fatter, so there might be outliers). If we had less than 5k entries, we would perform a Shapiro-Wilk Normality Test. I include the code in case you need it. # Using pre-installed library(MASS) # get distribution of studentized residuals (i.e. transform residuals for test) sresid &lt;- MASS::studres(simple_model) #using MASS package function to transform data easily shapiro.test(sample(sresid,5000)) # p value non-sign: normal distribution of residuals ## ## Shapiro-Wilk normality test ## ## data: sample(sresid, 5000) ## W = 0.93491, p-value &lt; 2.2e-16 11.3 Testing the Homoscedasticity Assumption 11.3.0.1 Plotting residuals Look at plots of residuals versus time and residuals versus predicted (i.e. fitted) value, and be alert for evidence of residuals that are getting larger (i.e., more spread-out) either as a function of time or as a function of the predicted value. (To be really thorough, you might also want to plot residuals versus some of the independent variables.) plot(simple_model, 1) Scale-location plot / spread-location plot (same as above, just using standardised residuals) We want a more or less horizontal line with more or less equally spread points around. plot(simple_model, 3) The red line does deviate slightly from being horizontal. There is no clear sign of heteroscedasticity (no clear deviation from horizontal line, no funnel shape of errors being larger where fitted values get larger). 11.3.0.2 Breusch-Pagan test # Breusch-Pagan test lmtest::bptest(simple_model) ## ## studentized Breusch-Pagan test ## ## data: simple_model ## BP = 57.792, df = 1, p-value = 2.914e-14 11.3.0.3 Whites test =&gt; This is a special case of the (simpler) Breusch-Pagan test. The only difference between Whites test and the Breusch-Pagan is that its auxiliary regression doesnt include cross-terms or the original squared variables. To show this more clearly, we will use two rather than one regressor in this code example. # General formula: # m &lt;- lm(A ~ B + C, data = dataset) # bptest(m, ~ B*C + I(B^2) + I(C^2), data = dataset) BMI2_model &lt;- lm(BMI ~ Poverty + PhysActiveDays, data = adults) bptest(BMI2_model, ~ Poverty*PhysActiveDays + I(Poverty^2) + I(PhysActiveDays^2), data = adults) ## ## studentized Breusch-Pagan test ## ## data: BMI2_model ## BP = 42.143, df = 5, p-value = 5.512e-08 The test suggests heteroscedasticity (i.e. residuals having a non-constant variance). The plots of residuals were not too worrying though. If we were quite concerned about heteroscedasticity, we could try using logarithmic or square root transformation on the response variable to reduce heteroscedasticity. 11.4 Test for autocorrelation (Violations of independence) Best test for residual autocorrelation: Inspect autocorrelation plot of the residuals. Pay especially close attention to significant correlations at the first couple of lags and around seasonal periods, because these are probably not due to chance. Given that there is no time variable in the dat, we will use an inbuilt data set, just for this function demo using population count from the US census: acf(uspop, lag.max = 15, plot = TRUE) 11.5 Collinearity A correlation matrix is probably the easiest starting point: model_corr_matrix &lt;- cor(nhanes %&gt;% select(Poverty, PhysActiveDays, Height), use = &quot;pairwise.complete.obs&quot;) model_corr_matrix ## Poverty PhysActiveDays Height ## Poverty 1.00000000 -0.01352413 0.16476155 ## PhysActiveDays -0.01352413 1.00000000 -0.02127384 ## Height 0.16476155 -0.02127384 1.00000000 There are several packages available for visualizing a correlation matrix in R. One of the most common is the corrplot function. Positive correlations are displayed in a blue scale while negative correlations are displayed in a red scale. Probably not needed for our examples, but useful if you have more variables: corrplot::corrplot(model_corr_matrix) 11.6 Multicollinearity Collinearity happens when two or more explanatory variables are correlated with each other. However, there is an extreme situation, called multicollinearity, where collinearity exists between three or more variables even if no pair of variables has a particularly high correlation. This means that there is redundancy between explanatory variables. Whilst collinearity can be detected with a correlation matrix, multicollinearity is not as easy to detect. The Variance Inflation Factor (VIF) can be used to find how much the variance of a regression coefficient is inflated due to multicollinearity in the model. The smallest possible value is one, indicating no multicollinearity. A value which exceeds 5 or 10 indicates a problematic amount of multicollinearity in the data. In R we use the vif() function from the car package to detect multicollinearity in a multiple regression model (where the response variable is ozone and all explanatory variables are added): # Define a regression model and save the model object as &quot;BMI3_model&quot; BMI3_model &lt;- lm(BMI ~ Poverty + PhysActiveDays + Height, data = adults) # Use the variance inflation factor from the car package car::vif(BMI3_model) ## Poverty PhysActiveDays Height ## 1.013563 1.001323 1.014225 All the variance inflation values are fairly close to one, suggesting our model doesnt have multicollinearity. If an explanatory variable has a high VIF, we usually remove that explanatory variable from our model. However, we need to look at how removing that variable affects the model. "],["homework-questions.html", "Chapter 12 Homework questions", " Chapter 12 Homework questions The code for sample solutions is given in the file Homework_solutions_day2 file. Please try to answer the following questions: Has the relationship between height and weight changed between the survey years? Is there a significant difference in how many rooms individuals accommodation has, between those who own and those who rent their accommodation? How are trouble sleeping and depression (both categorical) related? "],["survey-specific-functions.html", "Chapter 13 9. Survey specific functions", " Chapter 13 9. Survey specific functions As we showed you yesterday if we are using survey data that comes with weights, we need to account for those. The survey package has adapted functions for the different things that we did today. Lets first create the weights for the data at hand, specify the sample design and store it in an R object that we can then call in our tests. # add weighting variable nhanes &lt;- nhanes %&gt;% dplyr::mutate(WTMEC4YR = 0.5 * WTMEC2YR) # Create weighted R data object that holds the information about the sampling specifications nhanes_design &lt;- survey::svydesign(data = nhanes, strata = ~SDMVSTRA, id = ~SDMVPSU, nest = TRUE, weights = ~WTMEC4YR) Now we can seemlessly work with the weighted data. Lets create a frequency table that accounts for sample weights. table_gender_smoke_w &lt;- survey::svytable(~Gender + SmokeNow, design = nhanes_design) # contains estimated frequencies based on survey weights table_gender_smoke_w ## SmokeNow ## Gender No Yes ## female 24074780 19887290 ## male 29578392 24544477 For reference, there is some sample code for the other main analyses using the survey package. 13.0.0.1 Survey-weighted histogram # BMI ggplot(data = nhanes, mapping = aes(BMI, weight = WTMEC4YR)) + geom_histogram(binwidth = 2, color = &quot;white&quot;) + labs(x = &quot;BMI&quot;) ## Warning: Removed 2279 rows containing non-finite values (stat_bin). 13.0.0.2 Survey-weighted bar plot Bar plot of SleepHrsNight by gender # Compute the survey-weighted mean by Gender Sleep_mean_gender &lt;- survey::svyby(formula = ~SleepHrsNight, by = ~Gender, design = nhanes_design, FUN = svymean, na.rm = TRUE, keep.names = FALSE) # Construct a bar plot of average SleepHrsNight by gender ggplot(data = Sleep_mean_gender, mapping = aes(x = Gender, y = SleepHrsNight)) + geom_col() + labs(&quot;Average Hours of Sleep&quot;) 13.0.0.3 Survey-weighted Chi2 test Next, lets conduct a Chi2 test with survey weights survey::svychisq(~Gender + SmokeNow, design = nhanes_design, statistic = &quot;Chisq&quot;) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: survey::svychisq(~Gender + SmokeNow, design = nhanes_design, statistic = &quot;Chisq&quot;) ## X-squared = 0.025482, df = 1, p-value = 0.9573 13.0.0.4 Survey-weighted t-test # do mean sleeping hours differ between men and women svyttest(formula = SleepHrsNight~Gender, design = nhanes_design) ## Warning in summary.glm(g): observations with zero weight not used ## for calculating dispersion ## Warning in summary.glm(glm.object): observations with zero weight ## not used for calculating dispersion ## ## Design-based t-test ## ## data: SleepHrsNight ~ Gender ## t = -3.4077, df = 32, p-value = 0.001785 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -0.15287218 -0.04123256 ## sample estimates: ## difference in mean ## -0.09705237 # Let&#39;s get the mean values printed out (created in step prior to t-test) to aid interpretation of the t-test Sleep_mean_gender ## Gender SleepHrsNight se ## 1 female 6.976103 0.02374684 ## 2 male 6.879050 0.01953263 13.0.0.5 Survey-weighted scatter plot Lets look at the relationship between height and weight in 8 year old children. You can set the transparency parameter alpha to be the sample weight (WTMEC4YR). Observations with a bigger weight will show as darker points. ggplot(data = nhanes %&gt;% filter(Age == 8), mapping = aes(x = Height, y = Weight, alpha = WTMEC4YR)) + geom_point() + guides(alpha = FALSE) ## Warning: Removed 20 rows containing missing values (geom_point). 13.0.0.6 Survey-weighted regression Lets model Weight based on Height for 8 year olds model &lt;- svyglm(Weight ~ Height, data = nhanes %&gt;% filter(Age == 8), design = nhanes_design) summary(model) ## ## Call: ## svyglm(formula = Weight ~ Height, design = nhanes_design, data = nhanes %&gt;% ## filter(Age == 8)) ## ## Survey design: ## survey::svydesign(data = nhanes, strata = ~SDMVSTRA, id = ~SDMVPSU, ## nest = TRUE, weights = ~WTMEC4YR) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -92.1619 1.5032 -61.31 &lt;2e-16 *** ## Height 1.0181 0.0101 100.83 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 362.9893) ## ## Number of Fisher Scoring iterations: 2 "]]
