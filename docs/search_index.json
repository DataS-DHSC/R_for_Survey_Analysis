[["index.html", "Intermediate R - R for Survey Analysis Chapter 1 Index", " Intermediate R - R for Survey Analysis DHSC Analysts 2021-04-26 Chapter 1 Index Cover page goes here "],["introduction.html", "Chapter 2 Introduction 2.1 Aims 2.2 Why R", " Chapter 2 Introduction Course Notes for Intermediate R - R for Survey Analysis. 2.1 Aims Visualisation of the data processing workflow 2.1.1 Day 1 Load SAS/SPSS data in R. Clean the data. Exploring the data with summary stats. Applying survey weightings. Cross tabs. 2.1.2 Day 2 Plotting survey data. Regression &amp; plotting. Testing regression assumptions and hypothesis testing. 2.2 Why R (Youre here so hopefully youve been persuaded.) No licensing issues. More than 8 people can use it at once. Robustness - I can share the course notes and know it runs the same on your computer. Point-and-click SAS/SPSS has failed some scientific papers. Reuse - when solving the same problem you can copy-paste existing code. "],["day-1-1.html", "Chapter 3 Day 1 3.1 Outline for Day 1", " Chapter 3 Day 1 3.1 Outline for Day 1 We will cover: Loading survey data. Cleaning the data. Exploring the data. Applying survey weightings. Cross tabs. "],["loading-survey-data.html", "Chapter 4 Loading Survey Data 4.1 Analysing a survey in R 4.2 Loading the data 4.3 Cleaning the data 4.4 Exploring the data 4.5 Applying survey weighting for exploratory stats", " Chapter 4 Loading Survey Data 4.1 Analysing a survey in R Further Reading: R for Data Science, Hadley. 4.2 Loading the data As an example dataset well use the CDC National Health &amp; Nutrition Examination Survey. Its American, but its easier to access than the Health Survey for England. In RStudio create a new project, start a new script, and create a data/ folder. Download the demographic data file and the Body Measures data file to your data folder. Well load some libraries and the demographic data: library(tidyverse) library(haven) library(janitor) # Load demographic data nhanes &lt;- read_xpt(&quot;data/DEMO_J.XPT&quot;) And look at the first few rows: slice_head(nhanes, n=10) %&gt;% View() slice_head(nhanes, n=10) %&gt;% DT::datatable() We need the data dictionary to make sense of this. 4.3 Cleaning the data Cleaning data is long, and repetitive. best practise: clean it once, share the clean data. Example good-enough practise: keep the columns youre interested in, clean those. For 1-off analysis (2) is fair and proportionate. For weekly/monthly stats (1) is better - talk to Data Science team about RAP. 4.4 Exploring the data Weve already explored the data a little with View. This is perfectly valid. Hypothetical scenario - a stakeholder wants to know if targeting weight management services at demographics with lower education levels might improve health inequalities. Education level is in the demographics table, BMI is in the examination table. We want education &amp; participant ID from demographics, to join it with BMI &amp; participant ID from examinations. (Adult) education level is held in column DMDEDUC2. # recode Adult education nhanes &lt;- nhanes %&gt;% mutate(Education = case_when( DMDEDUC2 == 1 ~ &quot;Less than 9th grade&quot;, DMDEDUC2 == 2 ~ &quot;9-11th grade (Includes 12th grade with no diploma)&quot;, DMDEDUC2 == 3 ~ &quot;High school graduate/GED or equivalent&quot;, DMDEDUC2 == 4 ~ &quot;Some college or AA degree&quot;, DMDEDUC2 == 5 ~ &quot;College graduate or above&quot;, DMDEDUC2 == 7 ~ &quot;Refused&quot;, DMDEDUC2 == 9 ~ &quot;Don&#39;t Know&quot; )) %&gt;% select(ID = SEQN, Education) nhanes %&gt;% slice_head(n=10) %&gt;% DT::datatable() data dictionary for examination dataset # Load examination data exam &lt;- read_xpt(&quot;data/BMX_J.XPT&quot;) %&gt;% select(ID = SEQN, BMI = BMXBMI) exam %&gt;% slice_head(n = 10) %&gt;% DT::datatable() Joining them on ID: Refresher on joins nhanes &lt;- left_join(nhanes, exam, by=&quot;ID&quot;) nhanes %&gt;% slice_head(n=10) %&gt;% DT::datatable(nhanes) Keeping people with Education level recorded, &amp; valid BMI: nhanes %&gt;% filter(!is.na(Education), !is.na(BMI)) %&gt;% select(-ID) %&gt;% group_by(Education) %&gt;% summarise(average_BMI = mean(BMI)) %&gt;% knitr::kable() Education average_BMI 9-11th grade (Includes 12th grade with no diploma) 29.27825 College graduate or above 28.50249 Dont Know 31.11250 High school graduate/GED or equivalent 30.13217 Less than 9th grade 29.93982 Refused 30.20000 Some college or AA degree 30.82326 Refresh on filter Refresh on select Refresh on grouping &amp; summarising No obvious relationship there, but I didnt apply the survey weighting. 4.5 Applying survey weighting for exploratory stats In reality someone has tidied the NHANES data for R, so Ill load that. rm(exam, nhanes) # We&#39;re not using these data any more, we can remove them from memory. nhanes &lt;- NHANES::NHANESraw nhanes %&gt;% slice_head(n=10) %&gt;% DT::datatable() The survey weighting is WTMEC2YR, and we can summarise with weighted.mean.: nhanes %&gt;% filter(!is.na(Education), !is.na(BMI)) %&gt;% group_by(Education) %&gt;% summarise(average_BMI = weighted.mean(BMI, WTMEC2YR)) %&gt;% knitr::kable() Education average_BMI 8th Grade 29.22906 9 - 11th Grade 29.20260 High School 29.40650 Some College 29.17616 College Grad 27.50059 The manual page for weighted.mean can be viewed with ?weighted.mean or F1 when the cursor is inside weighted.mean. It looks like theres a distinction between college grads and non-college grads. "],["crosstabs.html", "Chapter 5 Crosstabs 5.1 Case 1: with survey weights 5.2 Case 2: Without survey weights", " Chapter 5 Crosstabs Two cases: 5.1 Case 1: with survey weights nhanes %&gt;% select(Gender, Education, wt=WTINT2YR) %&gt;% # There&#39;s a clever way to do this, but it&#39;s easier to write this 3 times. filter(!is.na(Gender), !is.na(wt), !is.na(Education)) %&gt;% group_by(Gender, Education) %&gt;% summarise(wt = sum(wt)) %&gt;% group_by(Gender) %&gt;% mutate(wt = scales::percent(wt/sum(wt))) %&gt;% pivot_wider(id_cols = c(Gender, Education), names_from=Gender, values_from=wt) %&gt;% knitr::kable() ## `summarise()` has grouped output by &#39;Gender&#39;. You can override using the `.groups` argument. Education female male 8th Grade 5.9% 6.24% 9 - 11th Grade 11.4% 12.14% High School 20.4% 22.62% Some College 32.8% 29.73% College Grad 29.4% 29.28% 5.2 Case 2: Without survey weights Using the janitor package. nhanes %&gt;% filter(!is.na(Education), !is.na(Gender)) %&gt;% tabyl(Education, Gender) %&gt;% adorn_percentages(&quot;col&quot;) %&gt;% adorn_pct_formatting(digits = 2) %&gt;% knitr::kable() Education female male 8th Grade 11.01% 11.47% 9 - 11th Grade 14.77% 15.65% High School 20.83% 23.37% Some College 31.00% 26.72% College Grad 22.39% 22.80% "],["loading-survey-data-1.html", "Chapter 6 Loading Survey Data 6.1 Analysing a survey in R 6.2 Loading the data 6.3 Cleaning the data 6.4 Exploring the data 6.5 Applying survey weighting for exploratory stats", " Chapter 6 Loading Survey Data 6.1 Analysing a survey in R Further Reading: R for Data Science, Hadley. 6.2 Loading the data As an example dataset well use the CDC National Health &amp; Nutrition Examination Survey. Its American, but its easier to access than the Health Survey for England. In RStudio create a new project, start a new script, and create a data/ folder. Download the demographic data file and the Body Measures data file to your data folder. Well load some libraries and the demographic data: library(tidyverse) library(haven) library(janitor) # Load demographic data nhanes &lt;- read_xpt(&quot;data/DEMO_J.XPT&quot;) And look at the first few rows: slice_head(nhanes, n=10) %&gt;% View() slice_head(nhanes, n=10) %&gt;% DT::datatable() We need the data dictionary to make sense of this. 6.3 Cleaning the data Cleaning data is long, and repetitive. best practise: clean it once, share the clean data. Example good-enough practise: keep the columns youre interested in, clean those. For 1-off analysis (2) is fair and proportionate. For weekly/monthly stats (1) is better - talk to Data Science team about RAP. 6.4 Exploring the data Weve already explored the data a little with View. This is perfectly valid. Hypothetical scenario - a stakeholder wants to know if targeting weight management services at demographics with lower education levels might improve health inequalities. Education level is in the demographics table, BMI is in the examination table. We want education &amp; participant ID from demographics, to join it with BMI &amp; participant ID from examinations. (Adult) education level is held in column DMDEDUC2. # recode Adult education nhanes &lt;- nhanes %&gt;% mutate(Education = case_when( DMDEDUC2 == 1 ~ &quot;Less than 9th grade&quot;, DMDEDUC2 == 2 ~ &quot;9-11th grade (Includes 12th grade with no diploma)&quot;, DMDEDUC2 == 3 ~ &quot;High school graduate/GED or equivalent&quot;, DMDEDUC2 == 4 ~ &quot;Some college or AA degree&quot;, DMDEDUC2 == 5 ~ &quot;College graduate or above&quot;, DMDEDUC2 == 7 ~ &quot;Refused&quot;, DMDEDUC2 == 9 ~ &quot;Don&#39;t Know&quot; )) %&gt;% select(ID = SEQN, Education) nhanes %&gt;% slice_head(n=10) %&gt;% DT::datatable() data dictionary for examination dataset # Load examination data exam &lt;- read_xpt(&quot;data/BMX_J.XPT&quot;) %&gt;% select(ID = SEQN, BMI = BMXBMI) exam %&gt;% slice_head(n = 10) %&gt;% DT::datatable() Joining them on ID: Refresher on joins nhanes &lt;- left_join(nhanes, exam, by=&quot;ID&quot;) nhanes %&gt;% slice_head(n=10) %&gt;% DT::datatable(nhanes) Keeping people with Education level recorded, &amp; valid BMI: nhanes %&gt;% filter(!is.na(Education), !is.na(BMI)) %&gt;% select(-ID) %&gt;% group_by(Education) %&gt;% summarise(average_BMI = mean(BMI)) %&gt;% knitr::kable() Education average_BMI 9-11th grade (Includes 12th grade with no diploma) 29.27825 College graduate or above 28.50249 Dont Know 31.11250 High school graduate/GED or equivalent 30.13217 Less than 9th grade 29.93982 Refused 30.20000 Some college or AA degree 30.82326 Refresh on filter Refresh on select Refresh on grouping &amp; summarising No obvious relationship there, but I didnt apply the survey weighting. 6.5 Applying survey weighting for exploratory stats In reality someone has tidied the NHANES data for R, so Ill load that. rm(exam, nhanes) # We&#39;re not using these data any more, we can remove them from memory. nhanes &lt;- NHANES::NHANESraw nhanes %&gt;% slice_head(n=10) %&gt;% DT::datatable() The survey weighting is WTMEC2YR, and we can summarise with weighted.mean.: nhanes %&gt;% filter(!is.na(Education), !is.na(BMI)) %&gt;% group_by(Education) %&gt;% summarise(average_BMI = weighted.mean(BMI, WTMEC2YR)) %&gt;% knitr::kable() Education average_BMI 8th Grade 29.22906 9 - 11th Grade 29.20260 High School 29.40650 Some College 29.17616 College Grad 27.50059 The manual page for weighted.mean can be viewed with ?weighted.mean or F1 when the cursor is inside weighted.mean. It looks like theres a distinction between college grads and non-college grads. "],["crosstabs-1.html", "Chapter 7 Crosstabs 7.1 Case 1: with survey weights 7.2 Case 2: Without survey weights", " Chapter 7 Crosstabs Two cases: 7.1 Case 1: with survey weights nhanes %&gt;% select(Gender, Education, wt=WTINT2YR) %&gt;% # There&#39;s a clever way to do this, but it&#39;s easier to write this 3 times. filter(!is.na(Gender), !is.na(wt), !is.na(Education)) %&gt;% group_by(Gender, Education) %&gt;% summarise(wt = sum(wt)) %&gt;% group_by(Gender) %&gt;% mutate(wt = scales::percent(wt/sum(wt))) %&gt;% pivot_wider(id_cols = c(Gender, Education), names_from=Gender, values_from=wt) %&gt;% knitr::kable() ## `summarise()` has grouped output by &#39;Gender&#39;. You can override using the `.groups` argument. Education female male 8th Grade 5.9% 6.24% 9 - 11th Grade 11.4% 12.14% High School 20.4% 22.62% Some College 32.8% 29.73% College Grad 29.4% 29.28% 7.2 Case 2: Without survey weights Using the janitor package. nhanes %&gt;% filter(!is.na(Education), !is.na(Gender)) %&gt;% tabyl(Education, Gender) %&gt;% adorn_percentages(&quot;col&quot;) %&gt;% adorn_pct_formatting(digits = 2) %&gt;% knitr::kable() Education female male 8th Grade 11.01% 11.47% 9 - 11th Grade 14.77% 15.65% High School 20.83% 23.37% Some College 31.00% 26.72% College Grad 22.39% 22.80% "],["day-2-1.html", "Chapter 8 Day 2 8.1 Outline for Day 2", " Chapter 8 Day 2 8.1 Outline for Day 2 Disclaimer: The focus of the training is not on understanding the statistical concepts, but rather on how to implement common tasks in R - with a few stats sprinkles. Im trying to show you the quick &amp; basic way of doing this and give you an idea of the fancier approaches - which gives you more flexibility (but also has a steeper learning curve). You can of course dive deeper on those topics or request follow-up training. :) Today we will cover the following topics: plotting (base R, ggplot2) regression models testing regression assumptions significance tests (chi2, t-test) 8.1.1 Take home messages to start with Work on a copy of your data in R - do not touch the raw data Work in R projects Structure your code with sections (Ctr shift R) help(x) or ?x is your friend when trying to understand function x scroll down to Examples in the helper window that pops up "],["loading-packages-and-data.html", "Chapter 9 Loading packages and data", " Chapter 9 Loading packages and data # load all required packages pacman::p_load(dplyr, readr, ggplot2, corrplot, NHANES, survey, broom, lmtest, car, knitr) # inspect data by calling head() function # head(nhanes) knitr::kable(head(nhanes),format = &#39;html&#39;) ID SurveyYr Gender Age AgeMonths Race1 Race3 Education MaritalStatus HHIncome HHIncomeMid Poverty HomeRooms HomeOwn Work Weight Length HeadCirc Height BMI BMICatUnder20yrs BMI_WHO Pulse BPSysAve BPDiaAve BPSys1 BPDia1 BPSys2 BPDia2 BPSys3 BPDia3 Testosterone DirectChol TotChol UrineVol1 UrineFlow1 UrineVol2 UrineFlow2 Diabetes DiabetesAge HealthGen DaysPhysHlthBad DaysMentHlthBad LittleInterest Depressed nPregnancies nBabies Age1stBaby SleepHrsNight SleepTrouble PhysActive PhysActiveDays TVHrsDay CompHrsDay TVHrsDayChild CompHrsDayChild Alcohol12PlusYr AlcoholDay AlcoholYear SmokeNow Smoke100 SmokeAge Marijuana AgeFirstMarij RegularMarij AgeRegMarij HardDrugs SexEver SexAge SexNumPartnLife SexNumPartYear SameSex SexOrientation WTINT2YR WTMEC2YR SDMVPSU SDMVSTRA PregnantNow 51624 2009_10 male 34 409 White NA High School Married 25000-34999 30000 1.36 6 Own NotWorking 87.4 NA NA 164.7 32.22 NA 30.0_plus 70 113 85 114 88 114 88 112 82 NA 1.29 3.49 352 NA NA NA No NA Good 0 15 Most Several NA NA NA 4 Yes No NA NA NA NA NA Yes NA 0 No Yes 18 Yes 17 No NA Yes Yes 16 8 1 No Heterosexual 80100.54 81528.77 1 83 NA 51625 2009_10 male 4 49 Other NA NA NA 20000-24999 22500 1.07 9 Own NA 17.0 NA NA 105.4 15.30 NA 12.0_18.5 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA No NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 4 1 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 53901.10 56995.04 2 79 NA 51626 2009_10 male 16 202 Black NA NA NA 45000-54999 50000 2.27 5 Own NotWorking 72.3 NA NA 181.3 22.00 NA 18.5_to_24.9 68 109 59 112 62 114 60 104 58 NA 1.55 4.97 281 0.415 NA NA No NA Vgood 2 0 NA NA NA NA NA 8 No Yes 5 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 13953.08 14509.28 1 84 NA 51627 2009_10 male 10 131 Black NA NA NA 20000-24999 22500 0.81 6 Rent NA 39.8 NA NA 147.8 18.22 NA 12.0_18.5 68 93 41 92 36 94 44 92 38 NA 1.89 4.16 139 1.078 NA NA No NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 1 1 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 11664.90 12041.64 2 86 NA 51628 2009_10 female 60 722 Black NA High School Widowed 10000-14999 12500 0.69 6 Rent NotWorking 116.8 NA NA 166.0 42.39 NA 30.0_plus 72 150 68 154 70 150 68 150 68 NA 1.16 5.22 30 0.476 246 2.51 Yes 56 Fair 20 25 Most Most 1 1 NA 4 No No NA NA NA NA NA No NA 0 Yes Yes 16 NA NA NA NA No Yes 15 4 NA No NA 20090.34 21000.34 2 75 NA 51629 2009_10 male 26 313 Mexican NA 9 - 11th Grade Married 25000-34999 30000 1.01 4 Rent Working 97.6 NA NA 173.0 32.61 NA 30.0_plus 72 104 49 102 50 104 48 104 50 NA 1.16 4.14 202 0.563 NA NA No NA Good 2 14 None Most NA NA NA 4 No Yes 2 NA NA NA NA Yes 19 48 No Yes 15 Yes 10 Yes 12 Yes Yes 9 10 1 No Heterosexual 22537.83 22633.58 1 88 NA dplyr::glimpse(nhanes) # if there are many columns (from dplyr package) ## Rows: 20,293 ## Columns: 78 ## $ ID &lt;int&gt; 51624, 51625, 51626, 51627, 51628, 51629, 51630, 51631, 51632, 51633, 51634, 51635, 51636, 51637~ ## $ SurveyYr &lt;fct&gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_1~ ## $ Gender &lt;fct&gt; male, male, male, male, female, male, female, female, male, male, male, male, male, female, male~ ## $ Age &lt;int&gt; 34, 4, 16, 10, 60, 26, 49, 1, 10, 80, 10, 80, 4, 35, 9, 4, 17, 13, 7, 42, 0, 66, 8, 45, 28, 8, 1~ ## $ AgeMonths &lt;int&gt; 409, 49, 202, 131, 722, 313, 596, 12, 124, NA, 121, NA, 48, 431, 115, 58, 208, 156, 85, 514, 9, ~ ## $ Race1 &lt;fct&gt; White, Other, Black, Black, Black, Mexican, White, White, Hispanic, White, Mexican, White, Other~ ## $ Race3 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ Education &lt;fct&gt; High School, NA, NA, NA, High School, 9 - 11th Grade, Some College, NA, NA, Some College, NA, 9 ~ ## $ MaritalStatus &lt;fct&gt; Married, NA, NA, NA, Widowed, Married, LivePartner, NA, NA, Married, NA, Widowed, NA, Married, N~ ## $ HHIncome &lt;fct&gt; 25000-34999, 20000-24999, 45000-54999, 20000-24999, 10000-14999, 25000-34999, 35000-44999, 35000~ ## $ HHIncomeMid &lt;int&gt; 30000, 22500, 50000, 22500, 12500, 30000, 40000, 40000, 70000, 17500, 22500, 17500, 30000, NA, 8~ ## $ Poverty &lt;dbl&gt; 1.36, 1.07, 2.27, 0.81, 0.69, 1.01, 1.91, 1.36, 2.68, 1.27, 0.93, 1.69, 1.36, NA, 1.84, 0.95, 0.~ ## $ HomeRooms &lt;int&gt; 6, 9, 5, 6, 6, 4, 5, 5, 7, 4, 5, 5, 7, NA, 6, 6, 5, 6, 4, 6, 9, 5, 7, 6, 4, 7, 5, 7, 5, 4, 5, 5,~ ## $ HomeOwn &lt;fct&gt; Own, Own, Own, Rent, Rent, Rent, Rent, Rent, Own, Own, Own, Own, Own, NA, Rent, Rent, Other, Ren~ ## $ Work &lt;fct&gt; NotWorking, NA, NotWorking, NA, NotWorking, Working, NotWorking, NA, NA, NotWorking, NA, NotWork~ ## $ Weight &lt;dbl&gt; 87.4, 17.0, 72.3, 39.8, 116.8, 97.6, 86.7, 9.4, 26.0, 79.1, 44.7, 89.6, NA, NA, 29.8, 17.9, 74.7~ ## $ Length &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 75.7, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 72.6, NA, NA, ~ ## $ HeadCirc &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ Height &lt;dbl&gt; 164.7, 105.4, 181.3, 147.8, 166.0, 173.0, 168.4, NA, 140.3, 174.3, 143.6, 180.1, NA, NA, 133.1, ~ ## $ BMI &lt;dbl&gt; 32.22, 15.30, 22.00, 18.22, 42.39, 32.61, 30.57, NA, 13.21, 26.04, 21.68, 27.62, NA, NA, 16.82, ~ ## $ BMICatUnder20yrs &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ BMI_WHO &lt;fct&gt; 30.0_plus, 12.0_18.5, 18.5_to_24.9, 12.0_18.5, 30.0_plus, 30.0_plus, 30.0_plus, NA, 12.0_18.5, 2~ ## $ Pulse &lt;int&gt; 70, NA, 68, 68, 72, 72, 86, NA, 70, 88, 84, 54, NA, NA, 82, NA, 68, 102, NA, 86, NA, 92, 72, 62,~ ## $ BPSysAve &lt;int&gt; 113, NA, 109, 93, 150, 104, 112, NA, 108, 139, 94, 121, NA, NA, 86, NA, 114, 112, NA, 116, NA, 1~ ## $ BPDiaAve &lt;int&gt; 85, NA, 59, 41, 68, 49, 75, NA, 53, 43, 45, 60, NA, NA, 47, NA, 78, 37, NA, 100, NA, 62, 37, 64,~ ## $ BPSys1 &lt;int&gt; 114, NA, 112, 92, 154, 102, 118, NA, 106, 142, 94, 126, NA, NA, 84, NA, 122, 106, NA, NA, NA, 14~ ## $ BPDia1 &lt;int&gt; 88, NA, 62, 36, 70, 50, 82, NA, 60, 62, 38, 62, NA, NA, 50, NA, 76, 12, NA, NA, NA, 68, 46, 62, ~ ## $ BPSys2 &lt;int&gt; 114, NA, 114, 94, 150, 104, 108, NA, 106, 140, 92, 124, NA, NA, 84, NA, 112, 110, NA, 116, NA, 1~ ## $ BPDia2 &lt;int&gt; 88, NA, 60, 44, 68, 48, 74, NA, 50, 46, 40, 62, NA, NA, 50, NA, 82, 38, NA, 100, NA, 64, 36, 68,~ ## $ BPSys3 &lt;int&gt; 112, NA, 104, 92, 150, 104, 116, NA, 110, 138, 96, 118, NA, NA, 88, NA, 116, 114, NA, NA, NA, 13~ ## $ BPDia3 &lt;int&gt; 82, NA, 58, 38, 68, 50, 76, NA, 56, 40, 50, 58, NA, NA, 44, NA, 74, 36, NA, NA, NA, 60, 38, 60, ~ ## $ Testosterone &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ DirectChol &lt;dbl&gt; 1.29, NA, 1.55, 1.89, 1.16, 1.16, 1.16, NA, 1.58, 1.94, 1.60, 1.27, NA, NA, 1.34, NA, 1.71, 1.63~ ## $ TotChol &lt;dbl&gt; 3.49, NA, 4.97, 4.16, 5.22, 4.14, 6.70, NA, 4.14, 4.71, 2.87, 3.83, NA, NA, 4.86, NA, 4.60, 5.04~ ## $ UrineVol1 &lt;int&gt; 352, NA, 281, 139, 30, 202, 77, NA, 39, 128, 109, 38, NA, NA, 123, NA, 315, 290, 60, 137, NA, 70~ ## $ UrineFlow1 &lt;dbl&gt; NA, NA, 0.415, 1.078, 0.476, 0.563, 0.094, NA, 0.300, 1.208, 0.956, 0.197, NA, NA, 1.538, NA, NA~ ## $ UrineVol2 &lt;int&gt; NA, NA, NA, NA, 246, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~ ## $ UrineFlow2 &lt;dbl&gt; NA, NA, NA, NA, 2.51, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA~ ## $ Diabetes &lt;fct&gt; No, No, No, No, Yes, No, No, No, No, No, No, Yes, No, No, No, No, No, No, No, Yes, NA, No, No, N~ ## $ DiabetesAge &lt;int&gt; NA, NA, NA, NA, 56, NA, NA, NA, NA, NA, NA, 70, NA, NA, NA, NA, NA, NA, NA, 34, NA, NA, NA, NA, ~ ## $ HealthGen &lt;fct&gt; Good, NA, Vgood, NA, Fair, Good, Good, NA, NA, Excellent, NA, Good, NA, NA, NA, NA, Good, Good, ~ ## $ DaysPhysHlthBad &lt;int&gt; 0, NA, 2, NA, 20, 2, 0, NA, NA, 0, NA, 0, NA, NA, NA, NA, 0, 0, NA, 0, NA, 0, NA, 0, 0, NA, NA, ~ ## $ DaysMentHlthBad &lt;int&gt; 15, NA, 0, NA, 25, 14, 10, NA, NA, 0, NA, 0, NA, NA, NA, NA, 18, 2, NA, 30, NA, 0, NA, 3, 0, NA,~ ## $ LittleInterest &lt;fct&gt; Most, NA, NA, NA, Most, None, Several, NA, NA, None, NA, None, NA, NA, NA, NA, NA, NA, NA, Sever~ ## $ Depressed &lt;fct&gt; Several, NA, NA, NA, Most, Most, Several, NA, NA, None, NA, None, NA, NA, NA, NA, NA, NA, NA, Mo~ ## $ nPregnancies &lt;int&gt; NA, NA, NA, NA, 1, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA, NA, 1, NA, ~ ## $ nBabies &lt;int&gt; NA, NA, NA, NA, 1, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, NA,~ ## $ Age1stBaby &lt;int&gt; NA, NA, NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 18, NA, NA, NA, NA, ~ ## $ SleepHrsNight &lt;int&gt; 4, NA, 8, NA, 4, 4, 8, NA, NA, 6, NA, 9, NA, 7, NA, NA, 7, NA, NA, 10, NA, 7, NA, 8, 8, NA, NA, ~ ## $ SleepTrouble &lt;fct&gt; Yes, NA, No, NA, No, No, Yes, NA, NA, No, NA, No, NA, No, NA, NA, No, NA, NA, Yes, NA, No, NA, N~ ## $ PhysActive &lt;fct&gt; No, NA, Yes, NA, No, Yes, No, NA, NA, Yes, NA, No, NA, No, NA, NA, Yes, Yes, NA, No, NA, No, NA,~ ## $ PhysActiveDays &lt;int&gt; NA, NA, 5, NA, NA, 2, NA, NA, NA, 4, NA, NA, NA, NA, NA, NA, 6, 2, NA, NA, NA, NA, NA, 5, 2, NA,~ ## $ TVHrsDay &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ CompHrsDay &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ TVHrsDayChild &lt;int&gt; NA, 4, NA, 1, NA, NA, NA, NA, 1, NA, 3, NA, 2, NA, 5, 2, NA, NA, 2, NA, NA, NA, 1, NA, NA, 3, 1,~ ## $ CompHrsDayChild &lt;int&gt; NA, 1, NA, 1, NA, NA, NA, NA, 0, NA, 0, NA, 1, NA, 0, 0, NA, NA, 6, NA, NA, NA, 6, NA, NA, 1, 0,~ ## $ Alcohol12PlusYr &lt;fct&gt; Yes, NA, NA, NA, No, Yes, Yes, NA, NA, Yes, NA, No, NA, NA, NA, NA, NA, NA, NA, No, NA, Yes, NA,~ ## $ AlcoholDay &lt;int&gt; NA, NA, NA, NA, NA, 19, 2, NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 8, NA, 3, 3, N~ ## $ AlcoholYear &lt;int&gt; 0, NA, NA, NA, 0, 48, 20, NA, NA, 52, NA, 0, NA, NA, NA, NA, NA, NA, NA, 0, NA, 104, NA, 52, 12,~ ## $ SmokeNow &lt;fct&gt; No, NA, NA, NA, Yes, No, Yes, NA, NA, No, NA, No, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA~ ## $ Smoke100 &lt;fct&gt; Yes, NA, NA, NA, Yes, Yes, Yes, NA, NA, Yes, NA, Yes, NA, No, NA, NA, NA, NA, NA, No, NA, No, NA~ ## $ SmokeAge &lt;int&gt; 18, NA, NA, NA, 16, 15, 38, NA, NA, 16, NA, 21, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ Marijuana &lt;fct&gt; Yes, NA, NA, NA, NA, Yes, Yes, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, No, NA, NA, NA, Y~ ## $ AgeFirstMarij &lt;int&gt; 17, NA, NA, NA, NA, 10, 18, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 13, ~ ## $ RegularMarij &lt;fct&gt; No, NA, NA, NA, NA, Yes, No, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, No,~ ## $ AgeRegMarij &lt;int&gt; NA, NA, NA, NA, NA, 12, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~ ## $ HardDrugs &lt;fct&gt; Yes, NA, NA, NA, No, Yes, Yes, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, No, NA, No, NA, N~ ## $ SexEver &lt;fct&gt; Yes, NA, NA, NA, Yes, Yes, Yes, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, Yes, NA, Yes, NA~ ## $ SexAge &lt;int&gt; 16, NA, NA, NA, 15, 9, 12, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 18, NA, 15, NA, 13, N~ ## $ SexNumPartnLife &lt;int&gt; 8, NA, NA, NA, 4, 10, 10, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 100, NA, 15, NA, 20, 0~ ## $ SexNumPartYear &lt;int&gt; 1, NA, NA, NA, NA, 1, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, 0, 0, NA~ ## $ SameSex &lt;fct&gt; No, NA, NA, NA, No, No, Yes, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, No, NA, No, NA, Yes~ ## $ SexOrientation &lt;fct&gt; Heterosexual, NA, NA, NA, NA, Heterosexual, Heterosexual, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA~ ## $ WTINT2YR &lt;dbl&gt; 80100.544, 53901.104, 13953.078, 11664.899, 20090.339, 22537.827, 74212.270, 23306.398, 8056.943~ ## $ WTMEC2YR &lt;dbl&gt; 81528.772, 56995.035, 14509.279, 12041.635, 21000.339, 22633.582, 74112.487, 24776.492, 8175.946~ ## $ SDMVPSU &lt;int&gt; 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, ~ ## $ SDMVSTRA &lt;int&gt; 83, 79, 84, 86, 75, 88, 85, 86, 88, 77, 86, 79, 84, 77, 88, 89, 81, 79, 88, 75, 80, 75, 77, 78, ~ ## $ PregnantNow &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, Unknown, NA, NA, NA, NA, NA, No, NA, NA, NA,~ # get a feeling for variables # categorical levels(nhanes$Depressed) ## [1] &quot;None&quot; &quot;Several&quot; &quot;Most&quot; table(nhanes$Depressed, useNA = &quot;ifany&quot;) ## ## None Several Most &lt;NA&gt; ## 7926 1774 814 9779 # numerical summary(nhanes$BMI, useNA = &quot;ifany&quot;) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 12.40 19.79 24.92 25.65 30.10 84.87 2279 "],["plotting-basic-charts-base-r.html", "Chapter 10 Plotting basic charts (base R) 10.1 histograms - basic frequencies 10.2 bar charts - plotting stats across categories 10.3 Box plots - plotting distribution of several categories/vars 10.4 Scatter plots - relationship between two continuous vars 10.5 Tiny statistics excursion 10.6 Are there any other plots that you regularly use?", " Chapter 10 Plotting basic charts (base R) It is important to distinguish between exploratory graphs and explanatory graphs: * Exploratory is done as part of analysis and there is no need to be pretty * Explanatory graphs are done once we understand the data and want to get insights across (for sharing with others) 10.1 histograms - basic frequencies # basic frequencies for numerical variables hist(nhanes$Height) hist(nhanes$Age) hist(nhanes$Age[nhanes$Age&lt;10]) # they sampled for both 0-6months and 6-12months 10.2 bar charts - plotting stats across categories # Great for plotting a statistic (e.g. mean value) for categorical data # Get dataframe into right format aggregates_by_health &lt;- nhanes %&gt;% dplyr::group_by(HealthGen) %&gt;% dplyr::summarise(count = n(), mean_weight = round(mean(Weight, na.rm=T),1), mean_age = round(mean(Age, na.rm=T),1)) # create bar plot to plot either Frequencies or aggregate statistics # Participant numbers by race barplot(count ~ HealthGen, data = aggregates_by_health) # average weight and age by race barplot(mean_weight ~ HealthGen, data = aggregates_by_health) 10.3 Box plots - plotting distribution of several categories/vars # Good for distribution of continuous data across categories boxplot(nhanes$Age ~ HealthGen, data = nhanes ) 10.4 Scatter plots - relationship between two continuous vars # Let&#39;s keep only adults in for this adults &lt;- nhanes %&gt;% dplyr::filter(Age &gt;= 18) # plot scatter plot plot(x = adults$Height, y = adults$Weight) Can you guess the Correlation coefficient? Whats the value of the correlation coefficient?\" cor(adults$Height, adults$Weight, use = &quot;pairwise.complete.obs&quot;) ## [1] 0.434507 Now lets add the line of best fit. First, we calculate slope and intercept of line of best fit coef(lm(Weight ~ Height, data = adults)) ## (Intercept) Height ## -72.209450 0.915329 Then we can add them to the plot # add them to plot (run both commands) plot(x = adults$Height, y = adults$Weight) # intercept and slope abline(-72.209450 , 0.915329) 10.5 Tiny statistics excursion Whats the relationship between the linear model regression coefficient and the correlation coefficient? hnorm &lt;- adults$Height/sd(adults$Height, na.rm = TRUE) wnorm &lt;- adults$Weight/sd(adults$Weight, na.rm = TRUE) df_norm &lt;- as.data.frame(cbind(hnorm,wnorm)) coef(lm(hnorm ~ wnorm, data = df_norm)) ## (Intercept) wnorm ## 14.7794973 0.4345723 Note: For correlation coef, the larger the value, the stronger the (linear!) relationship. For regression coefficients, a larger slope coefficient does NOT imply that the association between variables is stronger (remember the unit dependency). 10.6 Are there any other plots that you regularly use? "],["intermediate-plotting-in-r-ggplot2.html", "Chapter 11 Intermediate plotting in R (ggplot2) 11.1 Adding transparency 11.2 Adding automatic line of best fit 11.3 Adding colours 11.4 Fitting a line of best fit for each group of a categorical variable 11.5 Exercise: Brief in-class practice of making charts 11.6 ggplot cheat sheet", " Chapter 11 Intermediate plotting in R (ggplot2) This first bit of plotting was hopefully all pretty straight forward and the code was easy to read? In the R universe new packages are created all the time and ggplot2 is THE data viz package in R. It allows for more customisable ways of plotting - we can add layer, upon layer of extra info and change transparency and colours and much, much more. In a nutshell we have: DATA, AESTHETICS, GEOMETRIES. If you are interested in finding out more about how graphics are constructed with these three components, have a search around your favourite platform for Grammar of Graphics and check out the documentation of the ggplot2 package (https://ggplot2.tidyverse.org/). Lets have a look at what this means in practice. 11.1 Adding transparency # 2.1 Transparency ggplot(data = adults, aes(x = Height, y = Weight)) + geom_point(alpha = 0.4) ## Warning: Removed 580 rows containing missing values (geom_point). # explore the warning message # sum(is.na(adults$Weight)|is.na(adults$Height)) 11.2 Adding automatic line of best fit # 2.2 scatter plot with automatic line of best fit ggplot(data = adults, aes(x = Height, y = Weight)) + geom_point(alpha = 0.4) + geom_smooth(method = &quot;lm&quot;, se = FALSE) # se = TRUE would show standard error bars along the line 11.3 Adding colours Other cool things you can easily / automatically do with ggplot include colouring by category: # Define default colour scale suitable for colour-blind users scale_colour_discrete &lt;- ggthemes::scale_color_colorblind # plot in different colours based on the Gender variable ggplot(data = adults, aes(x = Height, y = Weight, colour = Gender)) + geom_point(alpha = 0.4) 11.4 Fitting a line of best fit for each group of a categorical variable # 2.4 get best fit line by Species ggplot(data = adults, aes(x = Height, y = Weight, color = Gender)) + geom_point(alpha = 0.4) + geom_smooth(method = &quot;lm&quot;, se = FALSE) 11.5 Exercise: Brief in-class practice of making charts Try out the simple commands for the charts that you most frequently produce in SPSS / SAS. Then choose a variable of interest that is continuous (e.g. Height or Pulse) and produce a box plot showing a break-down of distribution by a categorical variable of interest (e.g. HealthGen, MaritalStatus). 11.6 ggplot cheat sheet If youre lost or want to explore more options for plotting charts, you can consult a ggplot cheat sheet, e.g. https://www.rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf "],["regression-model.html", "Chapter 12 Regression model 12.1 Simple linear regression 12.2 Multiple linear regression 12.3 Lunch Break 12.4 ggplot cheat sheet reminder: 12.5 Regression output inspection (cont.)", " Chapter 12 Regression model 12.1 Simple linear regression The basic regression, i.e. linear model (lm), formula in R is: lm(formula = y ~ x, data = data_frame_name). Lets run a basic model to model BMI as a function of the Poverty indicator: # call model lm(BMI ~ Poverty, data = nhanes) ## ## Call: ## lm(formula = BMI ~ Poverty, data = nhanes) ## ## Coefficients: ## (Intercept) Poverty ## 25.2187 0.1856 Always save the model object - it allows you to access more of the outputs that are produced automatically when running the regression. If you only need the coefficients, call the coef function model_bmi_pov &lt;- lm(BMI ~ Poverty, data = nhanes) coef(model_bmi_pov) ## (Intercept) Poverty ## 25.2186624 0.1856308 Call the summary function on the model to see not only coefficients, but also the R-squared value, the associated standard error and p-value for each coefficient, the adjusted and the residual standard error. summary(model_bmi_pov) ## ## Call: ## lm(formula = BMI ~ Poverty, data = nhanes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.547 -5.896 -0.757 4.467 58.887 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.21866 0.10389 242.738 &lt; 2e-16 *** ## Poverty 0.18563 0.03747 4.954 7.36e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.784 on 16426 degrees of freedom ## (3865 observations deleted due to missingness) ## Multiple R-squared: 0.001492, Adjusted R-squared: 0.001431 ## F-statistic: 24.54 on 1 and 16426 DF, p-value: 7.36e-07 Inspecting model outputs: Once we have run a model, we often want to explore the fitted values and residuals (remember mean of resid = 0 and mean of fitted values = mean of actual y data). They are stored as part of the model object an can easily be accessed individually: #resid residuals(model_bmi_pov)[1:5] ## 1 2 3 4 5 ## 6.748880 -10.117287 -3.640044 -7.149023 17.043252 # fitted values fitted.values(model_bmi_pov)[1:5] ## 1 2 3 4 5 ## 25.47112 25.41729 25.64004 25.36902 25.34675 Explore all that is saved with your model To have a look at all the different aspects that are saved as part of your model, scroll down to Value section for list of all things part of your model: ?lm() 12.2 Multiple linear regression Bulding on the simple linear regression formula, the mulitple linear regression follows suit: lm(formula = y ~ x1 + x2 + x3, data = data_frame_name) # 1. save model - allows you to access more of the outputs model_mult &lt;- lm(BMI ~ Poverty + PhysActiveDays + SleepHrsNight + Diabetes + SmokeNow + Gender, data = adults) # 2. if you only need the coefficients, call the &#39;coef&#39; function coef(model_mult) ## (Intercept) Poverty PhysActiveDays SleepHrsNight DiabetesYes SmokeNowYes Gendermale ## 31.5376229 -0.1918165 -0.1567075 -0.2390670 2.9612169 -1.3255010 -0.8920421 # 3. Or call &#39;summary&#39; for a complete output summary(model_mult) ## ## Call: ## lm(formula = BMI ~ Poverty + PhysActiveDays + SleepHrsNight + ## Diabetes + SmokeNow + Gender, data = adults) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.682 -4.064 -0.695 3.216 33.250 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.53762 0.78469 40.191 &lt; 2e-16 *** ## Poverty -0.19182 0.08205 -2.338 0.019500 * ## PhysActiveDays -0.15671 0.06908 -2.268 0.023412 * ## SleepHrsNight -0.23907 0.09518 -2.512 0.012091 * ## DiabetesYes 2.96122 0.41912 7.065 2.22e-12 *** ## SmokeNowYes -1.32550 0.28006 -4.733 2.37e-06 *** ## Gendermale -0.89204 0.26941 -3.311 0.000946 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.803 on 1954 degrees of freedom ## (10430 observations deleted due to missingness) ## Multiple R-squared: 0.04844, Adjusted R-squared: 0.04552 ## F-statistic: 16.58 on 6 and 1954 DF, p-value: &lt; 2.2e-16 12.3 Lunch Break Try to answer the following questions drawing on what you have learned about creating tables, plots and regression analysis. Is the age in months variable actually the age in months, or just derived from age in years? How is age related to sleep? Which of these variables is most strongly associated with BMI in adults: age, poverty, or number of hours sleep? 3.b And how much of the variance in BMI can be explained through these variables? If youd like to go a bit further: Explore the dataset that we have provided to you and see if you can find variables that you are interested in. Try making a histogram, boxplot, scatter plot. Next pick a continuous variable that could contribute to overall reported XXX and run a linear model. Then plot the model (i.e. a scatter plot and a line of best fit). Also report how much of the variance in XXX is explained through your predictor variable. 12.4 ggplot cheat sheet reminder: If youre lost or want to explore more options for plotting charts, you can consult a ggplot cheat sheet, e.g. https://www.rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf 12.5 Regression output inspection (cont.) In the tidy universe of R there are a few functions that make your life easier &amp; prettier (after a steeper learning curve). In the broom package there are a few helpful functions that give you objects that you can not only read, but transform or save as neat csv files. The tidy function gives you a data frame of the coefficients (&amp; confidence intervals if you so choose) at the confidence level of your liking. # 1. save model - allows you to access all model outputs model_mult &lt;- lm(BMI ~ Poverty + PhysActiveDays + SleepHrsNight + Diabetes + SmokeNow + Gender, data = adults) # 2. Create tidy output broom::tidy(model_mult, conf.int = FALSE, conf.level = 0.95) ## # A tibble: 7 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 31.5 0.785 40.2 6.09e-258 ## 2 Poverty -0.192 0.0821 -2.34 1.95e- 2 ## 3 PhysActiveDays -0.157 0.0691 -2.27 2.34e- 2 ## 4 SleepHrsNight -0.239 0.0952 -2.51 1.21e- 2 ## 5 DiabetesYes 2.96 0.419 7.07 2.22e- 12 ## 6 SmokeNowYes -1.33 0.280 -4.73 2.37e- 6 ## 7 Gendermale -0.892 0.269 -3.31 9.46e- 4 The glance function gives you the model-level metrics: # Create tidy output broom::glance(model_mult) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 0.0484 0.0455 5.80 16.6 1.00e-18 6 -6227. 12470. 12515. 65795. 1954 1961 Now we can save the tidy output to easily further manipulate it or save it as a csv file. # Store tidy regression model output as object in R model_mult_tidy &lt;- broom::tidy(model_mult, conf.int = FALSE, conf.level = 0.95) # Save as a csv file at your chosen location (in &quot;outputs&quot; folder) and name in a meaningful way (e.g. BMI_model_output_p0.05.csv) readr::write_csv(model_mult_tidy, file = &quot;./outputs/BMI_model_output_p0.05.csv&quot;) 12.5.1 Try this for yourselves Using either your last regression model from the lunch break or a new one, play around with the broom package and try writing the outputs that you are interested in. Try including and excluding the confidence intervals and using different significance levels. "],["testing-regression-assumptions.html", "Chapter 13 Testing regression assumptions 13.1 Linear relationship 13.2 normality of residuals 13.3 Testing the Homoscedasticity Assumption 13.4 Test for autocorrelation (Violations of independence) 13.5 Collinearity 13.6 Multicollinearity", " Chapter 13 Testing regression assumptions In R, regression diagnostics plots (residual diagnostics plots) can be created using the base R function plot(). # define model simple_model &lt;- lm(Weight ~ Height, data = adults) # Change the panel layout to 2 x 2 (to look at all 4 plots at once) par(mfrow = c(2, 2)) # Use plot() function to create diagnostic plots plot(simple_model) The diagnostic plots show residuals in four different ways: Residuals vs Fitted: is used to check the assumptions of linearity. If the residuals are spread equally around a horizontal line without distinct patterns (red line is approximately horizontal at zero), that is a good indication of having a linear relationship. Normal Q-Q: is used to check the normality of residuals assumption. If the majority of the residuals follow the straight dashed line, then the assumption is fulfilled. Scale-Location: is used to check the homoscedasticity of residuals (equal variance of residuals). If the residuals are spread randomly and the see a horizontal line with equally (randomly) spread points, then the assumption is fulfilled. Residuals vs Leverage: is used to identify any influential value in our dataset. Influential values are extreme values that might influence the regression results when included or excluded from the analysis. Look for cases outside of a dashed line. 13.1 Linear relationship Ideally we want a horizontal line around zero: # Create the first diagnostic plot plot(simple_model,1) # Or plot the observed versus predicted values (again ideally a horizontal line) plot(simple_model$fitted.values, simple_model$model$BMI) Looks good so far. 13.2 normality of residuals Histogram of residuals hist(simple_model$residuals) QQ Plot of Residuals The QQ plot of residuals can be used to visually check the normality assumption. The normal probability plot of residuals should approximately follow a straight line. A bow-shaped pattern of deviations from the diagonal indicates that the residuals have excessive skewness (i.e., they are not symmetrically distributed, with too many large errors in the same direction). An S-shaped pattern of deviations indicates that the residuals have excessive kurtosisi.e., there are either too many or too few large errors in both directions. plot(simple_model, 2) The majority of the points fall approximately along the reference line, so we can assume normality. The endpoints are deviating from the straight line, suggesting a heavy-tailed distribution (Distribution is longer and tails are fatter, so there might be outliers). If we had less than 5k entries, we would perform a Shapiro-Wilk Normality Test. I include the code in case you need it. # Using pre-installed library(MASS) # get distribution of studentized residuals (i.e. transform residuals for test) sresid &lt;- MASS::studres(simple_model) #using MASS package function to transform data easily shapiro.test(sample(sresid,5000)) # p value non-sign: normal distribution of residuals ## ## Shapiro-Wilk normality test ## ## data: sample(sresid, 5000) ## W = 0.92894, p-value &lt; 2.2e-16 13.3 Testing the Homoscedasticity Assumption 13.3.0.1 Plotting residuals Look at plots of residuals versus time and residuals versus predicted (i.e. fitted) value, and be alert for evidence of residuals that are getting larger (i.e., more spread-out) either as a function of time or as a function of the predicted value. (To be really thorough, you might also want to plot residuals versus some of the independent variables.) plot(simple_model, 1) Scale-location plot / spread-location plot (same as above, just using standardised residuals) We want a more or less horizontal line with more or less equally spread points around. plot(simple_model, 3) The red line does deviate slightly from being horizontal. There is no clear sign of heteroscedasticity (no clear deviation from horizontal line, no funnel shape of errors being larger where fitted values get larger). 13.3.0.2 Breusch-Pagan test # Breusch-Pagan test lmtest::bptest(simple_model) ## ## studentized Breusch-Pagan test ## ## data: simple_model ## BP = 57.792, df = 1, p-value = 2.914e-14 13.3.0.3 Whites test =&gt; This is a special case of the (simpler) Breusch-Pagan test. The only difference between Whites test and the Breusch-Pagan is that its auxiliary regression doesnt include cross-terms or the original squared variables. To show this more clearly, we will use two rather than one regressor in this code example. # General formula: # m &lt;- lm(A ~ B + C, data = dataset) # bptest(m, ~ B*C + I(B^2) + I(C^2), data = dataset) BMI2_model &lt;- lm(BMI ~ Poverty + PhysActiveDays, data = adults) bptest(BMI2_model, ~ Poverty*PhysActiveDays + I(Poverty^2) + I(PhysActiveDays^2), data = adults) ## ## studentized Breusch-Pagan test ## ## data: BMI2_model ## BP = 42.143, df = 5, p-value = 5.512e-08 The test suggests heteroscedasticity (i.e. residuals having a non-constant variance). The plots of residuals were not too worrying though. If we were quite concerned about heteroscedasticity, we could try using logarithmic or square root transformation on the response variable to reduce heteroscedasticity. 13.4 Test for autocorrelation (Violations of independence) Best test for residual autocorrelation: Inspect autocorrelation plot of the residuals. Pay especially close attention to significant correlations at the first couple of lags and around seasonal periods, because these are probably not due to chance. Given that there is no time variable in the dat, we will use an inbuilt data set, just for this function demo using population count from the US census: acf(uspop, lag.max = 15, plot = TRUE) 13.5 Collinearity A correlation matrix is probably the easiest starting point: model_corr_matrix &lt;- cor(nhanes %&gt;% select(Poverty, PhysActiveDays, Height), use = &quot;pairwise.complete.obs&quot;) model_corr_matrix ## Poverty PhysActiveDays Height ## Poverty 1.00000000 -0.01352413 0.16476155 ## PhysActiveDays -0.01352413 1.00000000 -0.02127384 ## Height 0.16476155 -0.02127384 1.00000000 There are several packages available for visualizing a correlation matrix in R. One of the most common is the corrplot function. Positive correlations are displayed in a blue scale while negative correlations are displayed in a red scale. Probably not needed for our examples, but useful if you have more variables: corrplot::corrplot(model_corr_matrix) 13.6 Multicollinearity Collinearity happens when two or more explanatory variables are correlated with each other. However, there is an extreme situation, called multicollinearity, where collinearity exists between three or more variables even if no pair of variables has a particularly high correlation. This means that there is redundancy between explanatory variables. Whilst collinearity can be detected with a correlation matrix, multicollinearity is not as easy to detect. The Variance Inflation Factor (VIF) can be used to find how much the variance of a regression coefficient is inflated due to multicollinearity in the model. The smallest possible value is one, indicating no multicollinearity. A value which exceeds 5 or 10 indicates a problematic amount of multicollinearity in the data. In R we use the vif() function from the car package to detect multicollinearity in a multiple regression model (where the response variable is ozone and all explanatory variables are added): # Define a regression model and save the model object as &quot;BMI3_model&quot; BMI3_model &lt;- lm(BMI ~ Poverty + PhysActiveDays + Height, data = adults) # Use the variance inflation factor from the car package car::vif(BMI3_model) ## Poverty PhysActiveDays Height ## 1.013563 1.001323 1.014225 All the variance inflation values are fairly close to one, suggesting our model doesnt have multicollinearity. If an explanatory variable has a high VIF, we usually remove that explanatory variable from our model. However, we need to look at how removing that variable affects the model. "],["significance-tests.html", "Chapter 14 Significance tests 14.1 Chi-square 14.2 T-test", " Chapter 14 Significance tests 14.1 Chi-square Chi-square test for categorical variables determines whether there is a difference in the population proportions between two or more groups Lets look at smoking for men vs. women contingency_table_gender &lt;- table(adults$Gender, adults$SmokeNow) contingency_table_gender ## ## No Yes ## female 1116 1032 ## male 1663 1422 Or the dplyr way: adults %&gt;% dplyr::filter(!is.na(SmokeNow)) %&gt;% dplyr::group_by(Gender, SmokeNow) %&gt;% dplyr::summarise(n = n()) %&gt;% dplyr::mutate(freq = n / sum(n)) ## `summarise()` has grouped output by &#39;Gender&#39;. You can override using the `.groups` argument. ## # A tibble: 4 x 4 ## # Groups: Gender [2] ## Gender SmokeNow n freq ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 female No 1116 0.520 ## 2 female Yes 1032 0.480 ## 3 male No 1663 0.539 ## 4 male Yes 1422 0.461 chisq.test(adults$SmokeNow, adults$Gender) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: adults$SmokeNow and adults$Gender ## X-squared = 1.8573, df = 1, p-value = 0.1729 14.2 T-test You can specify whether you need it to be one-sided / two-sided &amp; one-sample / independent-sample. Lets look at whether BMI differs between men and women: hist(adults$BMI[adults$Gender==&quot;female&quot;]) hist(adults$BMI[adults$Gender==&quot;male&quot;]) Or lets do it the ggplot2 way # Define default colour scale suitable for colour-blind users scale_colour_discrete &lt;- ggthemes::scale_color_colorblind # Create plot facetted by Gender ggplot(adults, aes(BMI, fill = Gender)) + geom_histogram() + facet_wrap(.~Gender) ## Warning: Removed 580 rows containing non-finite values (stat_bin). Any guesses on whether there is a significant difference? Now its time to do the test. t.test(x = adults$BMI[adults$Gender == &quot;female&quot;], y = adults$BMI[adults$Gender == &quot;male&quot;], alternative = &quot;two.sided&quot;, # could also be &#39;less&#39; or &#39;greater&#39; for one-sided test paired = FALSE) ## ## Welch Two Sample t-test ## ## data: adults$BMI[adults$Gender == &quot;female&quot;] and adults$BMI[adults$Gender == &quot;male&quot;] ## t = 7.1026, df = 11386, p-value = 1.297e-12 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.6484638 1.1428217 ## sample estimates: ## mean of x mean of y ## 29.26902 28.37338 Yes, on average women seem to have a higher BMI in this survey. "],["homework-questions.html", "Chapter 15 Homework questions 15.1 Training evaluation", " Chapter 15 Homework questions The code for sample solutions is given in the file Homework_solutions_day2 file. Please try to answer the following questions: Has the relationship between height and weight changed between the survey years? Is there a significant difference in how many rooms individuals accommodation has, between those who own and those who rent their accommodation? How are trouble sleeping and depression (both categorical) related? 15.1 Training evaluation Please fill in this survey before you leave the call: https://forms.gle/J4Yt64C1FAncFFjW6 "],["extra-survey-specific-functions.html", "Chapter 16 Extra: Survey specific functions", " Chapter 16 Extra: Survey specific functions As James showed you yesterday if we are using survey data that comes with weights, we need to account for those. The survey package has adapted functions for the different things that we did today. Lets first create the weights for the data at hand, specify the sample design and store it in an R object that we can then call in our tests. # add weighting variable nhanes &lt;- nhanes %&gt;% dplyr::mutate(WTMEC4YR = 0.5 * WTMEC2YR) # Create weighted R data object that holds the information about the sampling specifications nhanes_design &lt;- survey::svydesign(data = nhanes, strata = ~SDMVSTRA, id = ~SDMVPSU, nest = TRUE, weights = ~WTMEC4YR) Now we can seemlessly work with the weighted data. Lets create a frequency table that accounts for sample weights. table_gender_smoke_w &lt;- survey::svytable(~Gender + SmokeNow, design = nhanes_design) # contains estimated frequencies based on survey weights table_gender_smoke_w ## SmokeNow ## Gender No Yes ## female 24074780 19887290 ## male 29578392 24544477 For reference, there is some sample code for the other main analyses using the survey package. 16.0.0.1 Survey-weighted histogram # BMI ggplot(data = nhanes, mapping = aes(BMI, weight = WTMEC4YR)) + geom_histogram(binwidth = 2, color = &quot;white&quot;) + labs(x = &quot;BMI&quot;) ## Warning: Removed 2279 rows containing non-finite values (stat_bin). 16.0.0.2 Survey-weighted bar plot Bar plot of SleepHrsNight by gender # Compute the survey-weighted mean by Gender Sleep_mean_gender &lt;- survey::svyby(formula = ~SleepHrsNight, by = ~Gender, design = nhanes_design, FUN = svymean, na.rm = TRUE, keep.names = FALSE) # Construct a bar plot of average SleepHrsNight by gender ggplot(data = Sleep_mean_gender, mapping = aes(x = Gender, y = SleepHrsNight)) + geom_col() + labs(&quot;Average Hours of Sleep&quot;) 16.0.0.3 Survey-weighted Chi2 test Next, lets conduct a Chi2 test with survey weights survey::svychisq(~Gender + SmokeNow, design = nhanes_design, statistic = &quot;Chisq&quot;) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: survey::svychisq(~Gender + SmokeNow, design = nhanes_design, statistic = &quot;Chisq&quot;) ## X-squared = 0.025482, df = 1, p-value = 0.9573 16.0.0.4 Survey-weighted t-test # do mean sleeping hours differ between men and women svyttest(formula = SleepHrsNight~Gender, design = nhanes_design) ## Warning in summary.glm(g): observations with zero weight not used for calculating dispersion ## Warning in summary.glm(glm.object): observations with zero weight not used for calculating dispersion ## ## Design-based t-test ## ## data: SleepHrsNight ~ Gender ## t = -3.4077, df = 32, p-value = 0.001785 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -0.15287218 -0.04123256 ## sample estimates: ## difference in mean ## -0.09705237 # Let&#39;s get the mean values printed out (created in step prior to t-test) to aid interpretation of the t-test Sleep_mean_gender ## Gender SleepHrsNight se ## 1 female 6.976103 0.02374684 ## 2 male 6.879050 0.01953263 16.0.0.5 Survey-weighted scatter plot Lets look at the relationship between height and weight in 8 year old children. You can set the transparency parameter alpha to be the sample weight (WTMEC4YR). Observations with a bigger weight will show as darker points. ggplot(data = nhanes %&gt;% filter(Age == 8), mapping = aes(x = Height, y = Weight, alpha = WTMEC4YR)) + geom_point() + guides(alpha = FALSE) ## Warning: Removed 20 rows containing missing values (geom_point). 16.0.0.6 Survey-weighted regression Lets model Weight based on Height for 8 year olds model &lt;- svyglm(Weight ~ Height, data = nhanes %&gt;% filter(Age == 8), design = nhanes_design) summary(model) ## ## Call: ## svyglm(formula = Weight ~ Height, design = nhanes_design, data = nhanes %&gt;% ## filter(Age == 8)) ## ## Survey design: ## survey::svydesign(data = nhanes, strata = ~SDMVSTRA, id = ~SDMVPSU, ## nest = TRUE, weights = ~WTMEC4YR) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -92.1619 1.5032 -61.31 &lt;2e-16 *** ## Height 1.0181 0.0101 100.83 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 362.9893) ## ## Number of Fisher Scoring iterations: 2 "]]
